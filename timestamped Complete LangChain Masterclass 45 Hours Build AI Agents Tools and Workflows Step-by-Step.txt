[00:00:00] Hello everyone, welcome to this brand new series on Langen, the framework that's changing how we build intelligent
[00:00:06] contextaware applications using live language models. If you have ever wondered how chatbots, AI agents, or
[00:00:13] retrieval based systems actually work behind the scenes, you're in the right place. In this series, we'll go beyond
[00:00:20] just theory. We'll build real world lang project step by step from simple
[00:00:25] conversational bots to AI systems that can use tools reason with data and even
[00:00:31] connect with external APIs and databases. By the end, you'll have the skills to design your own AI assistant
[00:00:38] or integrate LLM into your apps confidently. But before we dive in,
[00:00:43] let's understand what Langen really is. Langen is a framework that bridges LLMs
[00:00:49] with real data and real tasks. It helps developers manage prompts, memory,
[00:00:55] retrievalss, agents, and chains all in a structured modular way. Think of it as a
[00:01:01] toolbox that turns a large language model into powerful goal oriented system.
[00:01:08] In the upcoming videos, we'll explore langen concepts like chain, agents, retrieval, and vector stores and much
[00:01:14] more. Each topic will include hands-on coding demos and project based learning.
[00:01:20] So if you're excited to build real AI apps with Langen, make sure to subscribe, hit the bell icon, and follow
[00:01:26] the C series from the start. Let's unlock the true potential of language models together. I'm Abishek and this is
[00:01:33] the Langen series. Let's get started. So in this video, we're going to have our
[00:01:38] first interaction with an LLM. So for that, first of all, I'd like to create a virtual environment. So open a terminal
[00:01:45] in VS code. I've opened a folder in VS Code. Uh and then open a terminal. So
[00:01:51] we'll create our virtual environment virtual env. And I'm going to activate this virtual
[00:01:58] environment and install my libraries inside this. Okay. So the virtual environment is created and now I'm going
[00:02:05] to install some libraries here. So pip install
[00:02:10] langen google genai and I'll also install langen
[00:02:18] community and let's also have python
[00:02:25] do env or env. So let's install these three libraries.
[00:02:32] And while this is being installed I'm also going to create a requirements file
[00:02:37] requirements.tx txt and then mention these three libraries name here. So the first name is langchain
[00:02:45] community. Second is langchain google_jai.
[00:02:50] Well, you might be wondering why we're installing Google geni because uh the
[00:02:59] Google Google geni models or like the gemini models are kind of free. The flash
[00:03:06] models are free. So we will be using those models for our LLM interaction.
[00:03:11] I'm also going to create av file here file
[00:03:18] and inside this file we'll have our uh Gemini API key which will be defined as
[00:03:25] Google API key and then you can paste your Google API key in
[00:03:33] in this place here. So paste your key. Uh I'm going to replace
[00:03:39] replace my key here when I'm going to run the application here. Okay. So for the first first program or for the first
[00:03:47] interaction, we're going to create a pi file. But I would like to organize everything inside this single folder
[00:03:52] only for the future videos too. So let's just create a new folder and then
[00:03:59] type lm interaction and then inside this I am going to
[00:04:04] create a file called chatmodel uh_jemini
[00:04:09] dot py okay so we are inside our pi file and
[00:04:15] then this will be our first python program which which will also be
[00:04:22] our first program to interact with Gemini model. So for that I'm going to
[00:04:28] use the langen ecosystem. As you know that this is a series of langen. So we
[00:04:33] will import some libraries from langchen google_jai
[00:04:39] import chat google generative AI and then another library is from env import
[00:04:49] uh load env. So I'm going to call this function load
[00:04:55] env. This is going to load our Google API key uh from the env file and then
[00:05:02] I'm going to define my model. My model as I said will be a Gemini model. So for
[00:05:08] that I'm going to use a Gemini 2.5/model today.
[00:05:15] And let's also invoke our model here. So model.invoke invoke let's provide a
[00:05:21] prompt or like we can define a prompt here in string so I'll say what is the
[00:05:27] capital city of USA and then I'm going to invoke my model
[00:05:33] with this prompt here let's save this in result the model
[00:05:40] output will be saved in the result and then I want to print out the result here
[00:05:46] so let me clear this So now if I run my application here, I
[00:05:52] can directly run my Python file from this button here. So run Python file. It activates my virtual environment and
[00:05:59] then the Python file is being run. And you can see my result here. But we don't
[00:06:04] only get the result but we get a lot of things inside this. We get the answers.
[00:06:10] We also get some metadata from the response uh the number of tokens
[00:06:16] tokens and then uh stuff like that. What we only want here is our content part.
[00:06:21] So I'm going to hit result content. I'm going to clear this again and then run it. And we can see our result here. The
[00:06:30] capital city of USA is Washington DC. Well, this is our very first interaction
[00:06:37] with a LLM here. So throughout this throughout the course of this video, we're going to use um Gemini models as
[00:06:45] well as some hugging base models wherever applicable. We saw our very first interaction with an LLM. But I'll
[00:06:53] show you one problem that this LLM call has. So let me run this again
[00:07:04] or like let me run this again. We got the answer but
[00:07:10] I'm going to change the prompt here. So hello my name is Abishek.
[00:07:17] What is your name? So if I run this
[00:07:27] it gives me an output. So I'm a large language model and AI. I don't have a
[00:07:32] personal name like human does. You can just call me I assistant if you like. And then in the initials of this message
[00:07:38] it's written hello Abishek how can I help you today? So now if I type
[00:07:45] do you remember my name and then run this
[00:07:53] we'll see the problem here.
[00:07:59] As an AI I don't have memory of past conversation or personal information about you. So no I don't remember your
[00:08:04] name. So each and every LLM calls that we make are actually independent here.
[00:08:12] So how do we maintain these multi-turn conversations meaning the contextual conversation like we have in this chat
[00:08:19] GBT or like Gemini models. So we'll try to implement that here. So for that I'm
[00:08:25] going to create a new folder called chatbots.
[00:08:31] And inside this I'm going to create a new file chatbot.py.
[00:08:38] Uh
[00:08:47] so let me install a langen core if I haven't done it. So pp install lang gen
[00:08:54] core. Okay, I've already installed this. I can also maintain
[00:09:01] mention this in the requirement like gen core.
[00:09:07] So here I'm first going to import my Gemini model
[00:09:15] import chat Google generative model and then from langen core
[00:09:23] langen core dot messages I'm going to import system message human message and
[00:09:30] AI message. So in this chatbot we'll try to maintain that contextual information so that the
[00:09:38] AI remember our remembers our name. So from envoenv
[00:09:46] okay I'm going to load my credentials here. Then I'm going to define my model
[00:09:52] which will be chat Google generative AI. The model name is going to be Gemini
[00:10:01] 2.5/model.
[00:10:07] Now for the first message I will now define a chat history
[00:10:13] here which will be a list and inside this I'm first going to provide a system message and the system message is going
[00:10:21] to say that you are
[00:10:26] you are a helpful AI assistant.
[00:10:36] Okay. So I'll give this in loop so that I can continue talking with the bot. So
[00:10:43] I'm going to create a variable for user to type their
[00:10:48] message. So this is where user will be typing the message
[00:10:55] and then whatever message the user types I'm going to append this in my chat
[00:11:01] history so that so that the chat history list
[00:11:06] will have the information about what the conversation is being happening
[00:11:16] equals to user input. Now I also need a break from this chat.
[00:11:24] A way to break this chat. So user input dot lower equals to exit. If the user
[00:11:30] types exit and I'm going to break this or else I'm going to just invoke the
[00:11:36] model using my chatty.
[00:11:41] not just going to provide a single prompt to the user but I'm going to
[00:11:46] provide the model the entire contextual history of a conversation
[00:11:52] then the model is also going to reply back here so that will also be appended
[00:12:03] so chat do append and then it will be appended as an AI message
[00:12:09] where the content is going to be a result dot
[00:12:14] content. Okay. And I'm also going to print my bot
[00:12:21] response here. So result content.
[00:12:30] Finally, once the conversation breaks out, I'm going to print the entire chat history. We had chat history.
[00:12:39] Now if we run this we'll see that because of this particular
[00:12:46] list here where we have stored everything or every conversation that we had throughout this loop the model will
[00:12:54] be able to have some contextual information and at least remember our name or at least remember what we've
[00:12:59] talked about in the past. So let me clear this output and then run this again. Save this and run this again. Do
[00:13:06] I have any errors? Yeah. So there's a comma missing here.
[00:13:12] Cat history. Okay. So run run it.
[00:13:22] So it's asking for my message. I'm going to say hello.
[00:13:27] My name is Abishek.
[00:13:32] What is your name?
[00:13:42] Oh, okay. So, I probably gave the wrong model name here. Okay. So, I made a
[00:13:47] mistake here. So, let me clear this again. I'm going to recolor this. Gemini- 2.5- flash is the correct model
[00:13:55] name. So, let me run this again.
[00:14:00] Okay. So, hello. I'm shape.
[00:14:06] What is the name?
[00:14:12] So the model says, "Hello, Abishek, I do not have a name. I'm a large language model trained by Google." So let me also
[00:14:19] ask a few more questions. What are you trained on?
[00:14:26] So I've been trained by Google on a massive data set of text and code.
[00:14:33] uh this data set includes a wide variety of information from the internet and then and yeah stuff like that. So I'm
[00:14:39] going to ask another question. What is the capital of
[00:14:44] India? So the capital of India is New Delhi.
[00:14:50] Now we'll check if the bot remembers our name or not. Do you
[00:14:58] remember my name? And yes, it does remember my name
[00:15:03] because why it remembers our name is if we hit exit and then see our entire chat
[00:15:10] history, we can see that our entire chat history is being saved
[00:15:15] here. So this was the first message that we gave to the AI assistant. I asked
[00:15:22] this question. The AI replied me with this answer.
[00:15:29] I again asked another question using a human message and then just because of
[00:15:34] the existence of this contextual information the bot was able to remember what the
[00:15:41] conversation was about or like was able to remember the stuffs that we said in
[00:15:47] the past. So in the last video we saw how we can include contextual
[00:15:53] information in our chatbot. So in this video we're going to talk about something. We know what simple prompts
[00:16:00] are. We've already given prompts to our chatbot. But in this video we're going to introduce a concept called chat
[00:16:06] prompt template. So chat prompt template in lang chain. It is a tool that helps
[00:16:11] you design and organize prompt for chatbased AI models in a structured way. So instead of writing the entire message
[00:16:17] each time uh you can actually give a pattern that includes different roles like system human and AI along with
[00:16:25] placeholders for dynamic values. So such placeholders are actually later filled
[00:16:30] with real data when the prompt is actually used. So what this does is it
[00:16:36] makes it easier to maintain uh reuse and modify prompts uh without changing the
[00:16:41] main logic of your application. So if I say it in simple terms, it's like creating a reusable message template
[00:16:48] that guides how our chatbot or AI assistant uh should behave and respond
[00:16:53] in a conversation. So I'm going to include my chat chat prompt template in
[00:17:00] my next program. So I'm going to create a new file here to do.
[00:17:06] Prompt template.py. [Music] So
[00:17:13] have our import langen_core dotprompts
[00:17:19] import chat prompt template. I'll have my model to lang google geni
[00:17:26] import chat google generative AI and I'll also have my messages here. So from
[00:17:33] langen uh I'm not sure if I'll use this or not but we'll keep it for now. messages,
[00:17:40] input, system message, human message, and AI
[00:17:45] message. Okay, first of all, what we're going to do is I might not invoke the model at
[00:17:52] all. I'm just going to show you uh how our prompts are created using chat prompt template. Okay, so this is
[00:17:58] actually chat prompt template, not chat message prompt template. So I'm first
[00:18:04] going to create a template here. So chat template equals to chat prompt template.
[00:18:12] I'm going to create a list inside this. And then for our system message I'm
[00:18:19] going to type in type in this part and then you are a
[00:18:27] helpful domain expert. So the domain is going to
[00:18:35] be a placeholder which will be filled later on whenever we execute the stratum template. And for the human message
[00:18:43] I'm going to type explain in simple terms
[00:18:51] simple terms the concept of topic and then the topic is also going
[00:18:58] to be a placeholder here. So whenever I create a prompt, what I'm going to do is I'm going to use my chat template dot
[00:19:07] invoke and inside this since we have two different placeholders here. So I'm
[00:19:13] going to provide the value of those placeholders. The first one is domain. So we'll call this quantum quantum
[00:19:22] mechanics or quantum physics or something like that. And then the
[00:19:28] next is our topic. So I'm keeping the topic as warm.
[00:19:37] Okay. Now if I try and print this prompt, what I'm going to see is a
[00:19:44] generated prompt from this chat prompt template here where we have these two
[00:19:51] messages along with their placeholders. So let me clear this and then run this again.
[00:19:59] So if I run this I can see my
[00:20:05] I can see my prompts here. So the first one is a system message. You are a helpful quantum physics expert because
[00:20:12] the placeholder is being filled here and then uh the human message explain in simple terms the concept of wormhole. So
[00:20:20] if I need to invoke this I can directly define my model here. So if you define a model then you can directly invoke your
[00:20:27] model using this prompt here. So model dot invoke and
[00:20:33] then prompt I I think you can do this by yourself and then check out the result
[00:20:39] here. What I wanted to show you in this video is a new style of generating our
[00:20:45] prompts using this chat prom template. When building a chatbot, one of the
[00:20:51] biggest challenges is making it remember the conversation. You don't want to treat AI or you don't want the AI to
[00:20:58] treat every every user message as a brand new question. So that is where the message placeholder comes in. Uh it's
[00:21:06] like a dynamic container for a chat history. Instead of manually combining all previous messages into a single
[00:21:13] string every time you generate a prompt, you just put a placeholder in your template and lang fills it automatically
[00:21:21] uh with the actual conversation that happened so far. Uh so the placeholder doesn't just dump text, it preserves the
[00:21:28] role of each message whether it came from the human, the AI or the system. Uh
[00:21:34] so this means that uh the AI can actually uh identify between
[00:21:39] instructions, user prompts and is and its own pass responses. So that is what
[00:21:45] allows multi-turn conversations to feel natural that we see in chat GPT or
[00:21:50] Gemini interfaces that we use. So another powerful aspect of such is
[00:21:56] reusability. So you can define a single chat template with a message placeholder and it works for any number of previous
[00:22:03] conversations that we had. So whether the conversation has two turns or 20 turns, the placeholder always inserts
[00:22:09] the right history in the right format. So this also keeps the template clean and you don't need complex logic to
[00:22:16] actually stitch the chat together or like put the chat together. So finally message placeholders uh work seamlessly
[00:22:23] with lang chains role based messages like system message human message and AI message that we've already seen. So this
[00:22:31] combination actually uh uh ensures that our prompts are structured uh clear and
[00:22:39] and also are context aware which is actually essential when using LMS for chatbot or customer support agents. Uh
[00:22:47] so message placeholder is what lets a chatbot remember understand rules and
[00:22:53] continue conversation smoothly without you manually managing
[00:22:59] every previous line of dialogue. So that is what we're going to do in our
[00:23:04] upcoming code.
[00:23:11] So I'm going to create a new file called message placeholder
[00:23:17] holder.py. What I'm also going to do is create a new file that actually contains
[00:23:24] or saves my conversation here. So let me write it at chatbot history
[00:23:31] uh txt. As I told you that uh whenever we save our conversation, we save it in
[00:23:37] such a way that um that the information of the message is being preserved. So
[00:23:45] I'll put a few messages here. So the first one is a human message where it says that I want to request a refund for
[00:23:52] my order and the AI message uh says this thing. So okay I will use this and what
[00:23:58] I'm going to do is in my message placeholder I am going to
[00:24:03] load this chat and then create a new prompt based on my uh new user input. So
[00:24:11] first of all I'm just going to use from lang gen core dot promps
[00:24:17] import the first one is going to be chat prom template
[00:24:23] it should be promps. So chat prom template and then the other is going to be a message placeholder. So
[00:24:30] our chat template is going to be something that we've already seen.
[00:24:36] So chat template equals to chat prompt template.
[00:24:43] So this will contain a system message uh which says you are a very helpful
[00:24:53] helpful customer support agent
[00:24:59] and and then I'll also load our conversation history that we already have in this file. So for that I'm going
[00:25:06] to define a message placeholder with a variable name and then and then the
[00:25:12] variable name is going to be like uh chat
[00:25:19] history. It's just a variable name. It does not
[00:25:24] need to match with the file name provided here. And then the third is going to be a human message.
[00:25:32] a new human message that will come uh in this particular place here.
[00:25:40] So I'll also indicate this as a string. Okay. So this is our chat template. Now
[00:25:47] I'm going to load a chat history from a file. So for that I will create a list
[00:25:53] called chat history. So with open uh chat_history
[00:26:00] txt as file I can load a chat history chat history
[00:26:08] uh dot extend file
[00:26:13] dot read lines. Okay. Now
[00:26:20] uh I will create my new prompt here based on the chat template that I have
[00:26:28] as well as the chat history that I just loaded. So inside my chat history
[00:26:35] variable is going to be my chat history list.
[00:26:43] chat history list.
[00:26:49] Okay, so this should be inside curly braces. So chat history list and
[00:26:56] chat history along with this I'm going to also have a query because the query
[00:27:01] also contains a placeholder here. So in the query I'll ask where is my refund.
[00:27:09] So I can invoke this using a model too but I just want to show you the use of placeholder here. So I'm just going to
[00:27:14] print my prompt or my new prompt here. Okay. So let me rerun this and then we
[00:27:22] can see uh something's not right. Okay. So this
[00:27:27] should be txt. Clear this again and then run it.
[00:27:37] I definitely made some mistake here. Oh, it should be chatbot history, not
[00:27:44] chat history. Chatbot history.
[00:27:49] Okay, if I load this again, then we can see uh that the first message is a human
[00:27:57] message. uh you are a very helpful customer
[00:28:03] support agent. So far we focused on making AI conversations more natural and
[00:28:09] context aware. So we looked at how the AI can remember previous messages using features like message placeholder which
[00:28:16] allowed our chat bots to maintain multi-turn conversations like chat GPT or Gemini. Uh back then the AI could
[00:28:23] generate responses freely which worked well for casual conversation but the outputs were often unstructured
[00:28:29] sometimes too long inconsistent or sometimes hard to process automatically.
[00:28:34] Now we're moving into the next step which is structured responses. So instead of letting the AI respond
[00:28:40] however it wants, we want to define a schema that specifies the exact fields
[00:28:45] and types of information we want in the response. So this actually guides the AI to produce outputs that are predictable,
[00:28:52] organized and easy to work with. So for example, we can ask it to summarize the review and identify the sentiment and it
[00:28:59] will return the information in a clean and structured format. So the key idea
[00:29:04] is that structured output turns the AI from just a conversational partner into
[00:29:09] a reliable data source. So it complements the uh whatever we've done before. The AI still remembers the
[00:29:16] context of the conversation but now uh its responses are also consistent and u
[00:29:22] more readable making it much easier to integrate into applications dashboard or
[00:29:27] any system that needs such actionable data. So let me jump to the code and
[00:29:33] then show you what I mean by that concept here. So
[00:29:38] I'm going to create a new folder here which is going to be structured
[00:29:45] structured outputs. Let me give this as number three.
[00:29:53] So inside this I'm going to create a new file and then the file name is going to be
[00:29:58] structured output.py.
[00:30:05] So here again we're going to be using the geni model from Google. So import
[00:30:15] chat Google generative model. I'm also going to
[00:30:20] load my credentials. So env import load env. And here I'm going to introduce
[00:30:29] something new which is a typed dict type. So I'm going to load my credential
[00:30:39] define my model which is going to be a Gemini
[00:30:44] flashmodel. So as I said we want a proper structured
[00:30:53] output right? So for that we're going to define a schema. Let's write review.
[00:30:59] Uh so and the type is going to be typed tick. What we want is a summary which is
[00:31:07] going to be in string and a sentiment value
[00:31:12] uh which is going to be also be in a string. So it can be a positive,
[00:31:17] negative or sentiments like that. So now what I'm going to do is instead of
[00:31:24] directly invoking the model I'm going to convert this model output into a structured model output. So
[00:31:31] structured
[00:31:36] model equals to model dot with structured output
[00:31:42] and then I'm going to provide my review class here. So what this does is whenever this model is invoked, it tries
[00:31:50] to base its output on this particular structure provided here. Okay. So let me
[00:31:57] define my prompt. I'm not going to use a prompt uh chat prompt template. You know how to use that. So I'm just going to
[00:32:03] give my prompt here in a simple approach. So let's say this hardware is
[00:32:09] great but the the software feels
[00:32:18] kind of bloated. So many
[00:32:24] boilerplate boilerplate apps and my phone keeps
[00:32:33] hanging when I play PUBG.
[00:32:40] Okay. So this is going to be my review. uh and then based on this I'm going to
[00:32:47] invoke my model with a structured model dot invoke and then provide my prompt
[00:32:54] here and then I'm going to print my
[00:33:03] response. So now if I try to run this, let me run this.
[00:33:09] So what I'm going to see is I'm going to see a structured model output in this. So as I said we
[00:33:18] gave the model a schema to base its response upon and then the
[00:33:24] response is just based on the schema. Here we have a sentiment a sentiment and then the summary. So the sentiment is
[00:33:31] negative and then the summary is just hardware is great but software is bloated with bullet coursees and the
[00:33:37] phone hangs when playing PUBG. So it is just trying to summarize this particular
[00:33:42] provided review prompt here and then the response is quite well structured. Well,
[00:33:48] if I try to do something like this. So if I give a new prompt and then
[00:33:58] uh okay um I'll also show you what happens if we don't provide such structured outputs here. So my prompt is
[00:34:05] going to be uh generate
[00:34:11] sentiment and summary of
[00:34:17] the review below. Review given
[00:34:22] the review case. So if I paste my review here
[00:34:29] or I can just do this. Uh I can hit new prompt and then
[00:34:35] use this as f string
[00:34:41] and then include my review using this prompt placeholder.
[00:34:47] Well, if I invoke this with not my structured model but with my model, then
[00:34:53] I'm going to see that my response is going to be very much unstructured which I which I don't actually want. So I'm
[00:35:00] not going to print this for now. Let's not print this. Uh we'll just print it
[00:35:06] with the previous upper that we've done.
[00:35:12] Okay. Something's wrong. I'm not sure what. So let me check again. Uh
[00:35:22] ah okay. We've not used the function invoke here. So let me write in work
[00:35:29] clear this and run this again.
[00:35:41] I'm not printed by result in fact. So let me print by result too.
[00:35:56] So as you can see uh the response is quite mixed. Uh it says here's the
[00:36:03] sentiment and summary of the review. Sentiment mixed to negative positive the
[00:36:08] hardware is slightly highly praise and then yeah stuffs like that which is not actually structured.
[00:36:14] So we can actually base our LLM output
[00:36:20] response and then ask it to base it on on the provided schema. Here we can also
[00:36:27] include the details of of such schema but we'll do that in our upcoming video.
[00:36:33] In our previous video we explored structured outputs where the AI was guided to return information in a
[00:36:39] specific format like a summary and sentiment. That approach made the AI's
[00:36:44] response predictable and easy to use, but it was relatively simple. We were mostly extracting a summary and a single
[00:36:52] sentiment value. So in this next step, we taking structured output to the next
[00:36:57] level by introducing rich schemas with multiple fields, annotation and optional
[00:37:03] elements. Uh now instead of just a summary, we can ask the AI to extract multiple layers of information from a
[00:37:09] single piece of text. the key themes that is talked about, a concise summary,
[00:37:15] overall sentiment, and even detailed pros and cons. We're also telling the AI
[00:37:21] exactly how each field should be formatted and making it strict so it follows to the schema precisely. So,
[00:37:29] this approach transforms the AI into a powerful data extractor. uh you can feed
[00:37:35] it complex reviews, articles or even long form text and it will actually
[00:37:40] return you a structured machine readable object that you can directly use in
[00:37:46] dashboards, analytical tools or even databases. So the AI is no longer just
[00:37:51] answering your questions or chatting. Now it's systematically organizing knowledge in a way that applications can
[00:37:57] consume automatically. So by building on what we learned about contextual and structured output, uh the following uh
[00:38:05] implementation will show you how to scale from simple structured
[00:38:10] uh output to detailed multi- field extraction making AI truly practical for
[00:38:16] real world task like review analysis, product insights or content summarization. So let's go for the code
[00:38:22] now.
[00:38:28] Okay, I'm going to create a new file inside this detailed output structured.py.
[00:38:38] Uh I'm going to use these same fields, the same model that I have here. I'll
[00:38:45] just paste this. But but what I'm going to do is I'm going to add a few more uh
[00:38:52] fields inside the schema here. So let me first add key themes. Uh and then I'll
[00:38:59] also add an import of annotated and then optional here. So these two
[00:39:05] imports will be essential for me. So key themes will actually be uh annotated
[00:39:12] list of strings. So what we can also do is we can also
[00:39:18] write a detail about this field here. So we can say that
[00:39:24] uh key themes must write down all the
[00:39:30] themes all the important
[00:39:36] concepts discussed in the review in a list and in
[00:39:43] the summary I'm I'm again going to use an annotated key and inside this I'm
[00:39:49] going to Right. Must write down must write down a brief summary
[00:39:56] of the of the review. Uh
[00:40:01] close this uh inside my sentiment I'm again going to use an annotated.
[00:40:07] So with annotated I'm going to say must
[00:40:12] return a sentiment value
[00:40:20] uh or sentiment of the review either positive or
[00:40:27] negative
[00:40:32] and I'll also add pros and cons here. So, okay, I I need to close this bracket at
[00:40:40] last. Also, I need to do the same here. I'm also going to add pros and cons. So,
[00:40:46] pros is going to be annotated. But this is going to be an optional list. If the
[00:40:52] model does not find anything like pros and cons inside the review, then it is not going to put it or else if it finds
[00:40:58] something then then it will put it inside our uh pros list. So this is
[00:41:06] going to be optional uh str and then uh write down
[00:41:14] all the pros inside all the pros inside a list and also
[00:41:22] we'll do the same for cons annotated
[00:41:28] optional list of strings and this will also be
[00:41:35] write down all the cons inside
[00:41:41] a list. Okay. So I'm going to uh add some more prompts to this or some more
[00:41:48] details to this prompt here. So let me copy a prompt from somewhere. This prompt is from Google Pixel phone and it
[00:41:56] is quite detailed here. So you can just pause the video and then type it if you want. I'm going to perform this word rap
[00:42:04] for you so that it'll be easier for you to type this. So this is going to be my prompt and then based on this prompt uh
[00:42:11] let me remove this and then I will invoke a structured model here to see my
[00:42:18] result. So yeah so let me go ahead and then run
[00:42:25] this code. So if I run this as you can see with the structured
[00:42:30] output uh the model's response is going to be kind of structured here. So the
[00:42:36] key theme is Google 10 pixel pro GPU performance because it talks about the
[00:42:41] GPU performance somewhere around here with uh talking about its chipsets also
[00:42:49] it talks about the tensor G5 chipsets more gaming performance comparison with Pixel 9 Pro and things like that. The
[00:42:55] sentiment is mixed. It does not say positive or negative here. So
[00:43:03] we can also ask it to fix that. Uh but like it is okay for now. Uh the
[00:43:09] summary is also provided. And then the pros and cons, I don't think it's
[00:43:14] provided because the model could not find or the or the model could not
[00:43:20] identify the exact pros and cons in this particular review since this is optional. So the model could not write
[00:43:25] it or if we remove optional from here and then ask the model that it has to
[00:43:31] include the pros and cons or the or like just the pros the model will do that and
[00:43:37] then include all the pros of the review provided. So let's run this again
[00:43:48] and then we see that uh summary is provided.
[00:43:54] Uh okay what else is provided here? Summary is provided key things is key things is provided sentiment is provided
[00:44:00] and then it should also contain pro somewhere. Let me copy this and then see
[00:44:06] it in my uh copy. Let's open a JSON formatter
[00:44:14] window if we can see the pros or not.
[00:44:22] So JSON formatter [Music]
[00:44:29] paste this paste my JSON here and and then we see that it still hasn't generated any pros here but um that's
[00:44:37] okay for now still our response is kind of structured here. So this is how we
[00:44:43] can take our structured response to somewhat another level where we can include the details of what each of
[00:44:51] these keys that we want that we want to include in the responses mean and then
[00:44:56] what format should they be provided in. In our previous video, we explored
[00:45:01] structured outputs where the AI was guided to extract specific fields like summary or sentiment. That approach
[00:45:08] worked well but the schema was relatively simple. Uh now we're taking
[00:45:13] structured output a step further by using pyentic models to define a rich detailed schema. So Pyntic is a Python
[00:45:21] library that allows us to create data models with type validation default values optional fields and and clear uh
[00:45:30] details. So this means that we can tell the AI exactly what kind of data we expect for each field and we can trust
[00:45:36] that the output will confirm to these types here. So by integrating pyic with
[00:45:42] lang chain structured outputs uh we can define multiple fields like key themes a brief summary sentiment pros cons and
[00:45:49] even the reviewer's name with strict validation. Um, the AI then produces the
[00:45:55] responses that are not just structured but also type safe, consistent and fully compatible with Python applications.
[00:46:03] So this is especially useful when dealing with long or complex text. So instead of uh getting unstructured
[00:46:10] paragraphs, the AI learns to organize information into a machine readable object that can be directly used in
[00:46:16] dashboards, analytical tools or database. And because of Pythics validation, we reduce the risk of errors
[00:46:22] like missing fields or wrong data types too. So the integration takes structured outputs from simple field extraction to
[00:46:28] a robust, reliable and a developer friendly approach. Uh it also kind of ensures that the AI not only remembers
[00:46:35] the conversation context but also returns precise, actionable and validated information
[00:46:41] making it ready for uh real world applications. So let's go to the code
[00:46:47] now. So I'm going to create a new file here
[00:46:56] or this is going to be uh sorry three pi dantic structured
[00:47:06] structured output dopy.
[00:47:11] Okay. So let's start. As always, we're going to use the same
[00:47:16] geni model. import chat Google generative AI. I'm going to install uh
[00:47:24] one more library here which is called pyante. So included in requirements.ext text and
[00:47:30] then I can directly install this using this command
[00:47:35] and I also okay that is all for now because I don't have any emails right
[00:47:40] now or else we would have to have to install some other libraries too. So what I'm going to do is from env
[00:47:48] import load env uh from typing
[00:47:55] uh import digit Sorry, import
[00:48:04] date. I I'll also include an annotated literal
[00:48:09] uh and also a keyword called optional. Not sure I'm going to use each and every one of these though. But still uh we'll
[00:48:16] keep it in the import. So I'll import from pyic import base model.
[00:48:23] And then I'm going to load my credential. I'm going to define my model.
[00:48:29] Uh my model will be Gemini 2.5 slash.
[00:48:39] Now I'm going to define a similar schema like we did like we did in earlier code.
[00:48:45] So I'm going to define a class which will be called review and the import will not be a type but it will be a base
[00:48:52] model from our pendic class. So like I said earlier, I'm going to use
[00:48:59] uh use a keyword called key themes.
[00:49:04] And instead of doing like uh annotated list
[00:49:10] of list of our content, what we'll do is we'll include
[00:49:16] a class called field here and then define this as a field. So this is going to be a a list of string. Uh its type is
[00:49:27] going to be list of string and its detail will be included in the field value. So I can include a field and then
[00:49:34] write down the details here. So write down three key themes
[00:49:42] discussed in the review
[00:49:48] in a list. Uh the next is going to be my summary.
[00:49:55] My summary is going to be a string. And then I'll again write a field to write down the details of that sum of that
[00:50:02] summary keyword. Now this is going to be a brief summary
[00:50:08] of of the review. I'll also write sentiment.
[00:50:15] And in case of sentiment, we had something like a mix sentiment earlier. But I'm going to fix the model or ask
[00:50:22] the model to give either positive or negative sentiment here. So for that I'm going to use literal. So one will be
[00:50:28] positive. One option will be positive and then the other option is going to be
[00:50:34] a negative literal. So for that I'm also going to write down the details.
[00:50:42] Uh description equals to return
[00:50:48] the sentiment of the sentiment of the review either
[00:50:57] positive or negative. And this is going to guide me guide my
[00:51:03] model to return either positive or negative here. Now uh I'll also include
[00:51:09] a name here. So name is going to be in string and then I'll keep an optional
[00:51:14] keyword here because the names might not be available uh in the review but if it is then uh
[00:51:21] the model will definitely show the name too. So the description is going to be the name of the
[00:51:29] reviewer or or I'm going to say write down write down the name of the reviewer.
[00:51:38] Okay. So this is my output schema that I want my model to provide my output with.
[00:51:43] So I'm going to create a structured model. So this is going to be model dot
[00:51:51] with structured output and then I'll provide my class and then
[00:51:58] uh I'll also instruct this instruct the model to strictly follow
[00:52:04] the provided schema. Now uh let me provide my prompt. I can copy
[00:52:12] my prompt from the previous video or the previous code that I had. So I'm going
[00:52:18] I'm just going to copy everything else from now on here. So it is going to be a prompt and then my prompt will be
[00:52:25] invoked using the structure model and the code is complete. So let's run
[00:52:32] this code. Let me check if everything is all right. Okay, it seems okay. So let me run this. Uh when we run this,
[00:52:41] the model output will be based strictly on the schema as provided earlier.
[00:52:56] Okay. So we have key themes. We have three key themes here. One, two, and
[00:53:02] three key themes. We also have a summary. Uh and then and then okay what
[00:53:08] else? Uh we have a sentiment which is called which is strictly following the
[00:53:15] uh literal or the option that I provided here and then the name is actually not specified. So if I change this prompt
[00:53:24] and then add a few text like add a name to this reviewed by Abishek then I'll
[00:53:31] see that the model also includes my name
[00:53:37] here. It should include my name here. Let's see the output. So yeah you can see the name is being included and then
[00:53:44] everything else is present based on the schema that we provided. In this video,
[00:53:49] we're exploring a powerful combination of langen features that help us control,
[00:53:55] structure, and process AI outputs in multi-step workflows. So there are three
[00:54:00] key concepts at player. One is prompt template. Second one is str output parser and and the third one is
[00:54:07] chaining. So first let's talk about prompt template. So prompt template allows us to create reusable structured
[00:54:14] prompts with placeholders for dynamic content. Instead of hard- coding every instruction for the AI, we can define a
[00:54:22] template once specifying exactly what we want and then fill in fill in the variables as needed. For example, we can
[00:54:30] have one template that asks the AI to write a detailed report on any topic and then another template that takes the
[00:54:37] generated block of text from the previous prompt and then generate a concise summary about it. So prompt
[00:54:44] templates gives us the consistency and control over how we interact with the AI which is crucial for predictable
[00:54:51] results. The next concept we have is STR output parser. So whenever the AI
[00:54:56] generates text the raw output can be inconsistent. It can have extra lines,
[00:55:01] unexpected formatting or maybe some minor variations in the wording. So str
[00:55:07] output parser acts like this cleaning and standardizing tool and it also
[00:55:12] ensures that uh no matter how the AI phrases its output, the output is
[00:55:18] converted into a neat predictable string that we can feed into the next step
[00:55:23] directly. So this is especially important whenever we are building
[00:55:30] multi-step workflows one after the other because one wrong uh output can easily
[00:55:37] break the next prompt or the step in the process. Now whenever we talk about multi-step workflows
[00:55:44] we introduce this concept of chaining where the output of one prompt becomes
[00:55:49] the input to the next. So by connecting this multiple prompts in a chain, what
[00:55:54] we can do is we can perform complex task that requires several steps. For instance, let's say we first generate a
[00:56:02] detailed report on a topic using one template, parse it into a clean string using str output parser and then feed it
[00:56:09] into another template to create this short concise memory. Now this chain ensures that the flow of
[00:56:15] information is seamless while templates and parsers gives us the control and reliability at each step. To summarize,
[00:56:24] the approach allows us to uh break down or decompose this complex AI task into structure and
[00:56:31] manageable steps. To summarize these three key concepts, prompt templates
[00:56:36] defines what we want and keep instructions consistent. STR output
[00:56:41] parser ensures clean and reliable outputs and chaining lets us link multiple steps together to produce more
[00:56:48] advanced result. Now when we combine these three together we can build these
[00:56:54] multi-step AI workflows for real world application. So now let's go into the practical
[00:57:02] implementation of these three concepts that we have just talked about.
[00:57:08] Okay. Okay. So, I'm going to create a folder here
[00:57:15] output parsers. Let's create a file inside this str
[00:57:22] output parsers. So, let's import
[00:57:30] a model first. Import chat Google generative AI. Let's also
[00:57:37] import env uh and then we'll import
[00:57:46] prompts from langen core
[00:57:53] prompt template sorry prompt template okay we'll do this first we'll show how
[00:58:00] we break down the steps and then we'll link up together uh using chaining at last So first of all I'm going to define
[00:58:07] a model. The model is the Gemini flash model.
[00:58:16] Okay. Now uh I will generate my first prompt
[00:58:23] using prompt templates. So first prompt
[00:58:28] we'll define this using template one. This is done using prompt template.
[00:58:35] The template detail is write a detailed
[00:58:40] report on topic. The topic is going to be my
[00:58:46] placeholder. But what will come inside that placeholder is this input variable.
[00:58:52] And this input variable is going to be my topic.
[00:58:58] Okay. If I need to invoke this, I can invoke this using this prompt one template one.
[00:59:08] Uh the topic is let's say uh English
[00:59:14] Premier League. English Premier League 2024
[00:59:21] or 2023 2024.
[00:59:28] Okay. So this is my topic. If we want to check what kind of prompt
[00:59:35] it's it generates, we can run this and then see
[00:59:42] that the that using the prompt template, we can generate such kind of prompts here. Right? Real report on English
[00:59:48] Premier League 2023 2024. Simply this particular topic is going to be replaced
[00:59:55] here in the placeholder. Okay, I don't want to print this. Now I want to generate my second prompt.
[01:00:03] Second prompt, we'll define it using template 2. We'll use prompt template.
[01:00:09] And then the template details is going to be write a four point summary
[01:00:18] on the following. The placeholder is going to be text.
[01:00:25] uh now after this what I'm going to do is I'm going to define my input variables which will be my text here. So
[01:00:33] based on this I can again generate prompt two. So prompt two is going to be invoke from template 2.
[01:00:41] Template 2 dot invoke and then my text
[01:00:46] will be uh okay where will my text come from? Okay,
[01:00:52] my text has to come from this first prompt here. So for that I will have to
[01:00:58] invoke the model using this particular prompt here. So model dot invoke. Okay,
[01:01:03] let's write this result one. And then model dot invoke uh this has to
[01:01:10] be prompt one and then we need the content of this one not the entire
[01:01:17] response but only the content part or the text part. So here what I'll have to put is I'll have to put rest
[01:01:26] result one. Okay. Now based on this I'll have to
[01:01:32] again invoke the model
[01:01:38] prompt to uh
[01:01:44] prompt two. But now I can print
[01:01:50] result content here. Okay. So here we've only used prompt template
[01:01:56] the first concept that we've talked about. So we can define a template in such a way and then generate the prompt
[01:02:03] based on uh based on the template that we've created using prompt template and
[01:02:08] then invoke the model. So, but one thing that you need to remember here is result
[01:02:14] one if we only invoke the model and not print its content. This is not going to
[01:02:19] give me a standard string response here. So for that what I need is I want to
[01:02:27] take out the content part of this result one. But still I might not be sure that
[01:02:32] this provides me a string here. For that I'm going to do str result just to be on
[01:02:38] the safer side because the prompt needs to be a result here or a string here
[01:02:44] because the content we pass in this particular text has to be a string. So now what we can do is we can print out
[01:02:51] result one to here and then we'll run this model first then we'll invoke invoke str output parser and jin
[01:02:59] later on. Let's also use a divider so that it becomes clear to us. Okay.
[01:03:07] Now if I run this, I don't think I have any errors. Let me run this.
[01:03:26] [Music]
[01:03:39] Okay, let's go down up here.
[01:03:52] So here we have the here we have the result from the first
[01:03:57] prompt. So it is in in fact giving me in terms of points. So this is a detailed
[01:04:06] detailed uh report about English Premier League. And then from
[01:04:12] the second prompt we have a fourpoint summary about the English Premier League. It is summarizing whatever text is being generated upwards here. Okay,
[01:04:19] this is working well. But we could have made this code a lot
[01:04:24] shorter here. So how do we do that? Now we introduce the concept of str output parser to make sure that our output is
[01:04:32] in string and then based on that we also introduce the
[01:04:38] concept of chaining so that we don't have to write two different inv works here. So now we're going to use that
[01:04:44] particular approach. So I don't need to clear anything much. I'm going to
[01:04:50] comment this out. I'm also going to comment these two lines out.
[01:04:55] Uh I'll have my prompt one.
[01:05:03] In fact, I don't even need to generate prompt one, prompt two. So I'll just
[01:05:08] have two different templates. I'll uh import uh str output parser here. So
[01:05:15] langen core dot output parsers import str output parser.
[01:05:21] Okay, I'm going to remove all of this so that you don't get confused here. So I don't need this. I don't need this. What
[01:05:28] I'll have to define is a chain. So my chain is going to be
[01:05:34] first invoke template one then invoke a model. Okay, I also need to define a parser
[01:05:40] here. So, parser object from str output parser class. Okay, so the model
[01:05:48] response needs to be converted into a standard string format using the str output parser. Then after that, I'll
[01:05:54] have to invoke model uh template 2 first. Then again, I'll have to invoke
[01:06:00] the model, pass the prompt from template to the model, and then use a parser again. So all of those steps can be
[01:06:07] reduced to this single step using the concept of chain. Now to get the result what I need to do
[01:06:14] is I just need to invoke the chain and not the model because the model invocation process is already defined
[01:06:20] inside this chain here. So so
[01:06:25] what parameter do we need to provide here? If we look at template one the entry point to template one is this
[01:06:31] particular topic here. So what we need to do is we just need to provide a topic inside this.
[01:06:37] The topic can be English Premier League 2023
[01:06:44] 2024 and then I can simply provide print out the result inside this and still the
[01:06:51] model will work but the model will only print out the output of this particular
[01:06:59] second template here. All of these in between task will be handled by this chain. So let's run this.
[01:07:07] It will take some time to run because there are two model invocations in between.
[01:07:42] So as you can see we can directly see the output of this particular second
[01:07:47] template here because everything in between is being handled by this chain and the final result is a fourpoint
[01:07:53] summary that we get from this particular topic here. So what is happening here?
[01:08:00] Everything that we did earlier has been happening here too. But the
[01:08:06] but since we're only printing the final result, we're seeing the final result
[01:08:11] here. So this concept of chain is very crucial in implementing such multi-step
[01:08:17] workflows that we talked about earlier. In our previous video, we learned how to use prompt templates to give the model
[01:08:24] clear instructions and how to use str output parser to clean up the model's text responses. But what if we want the
[01:08:32] model to not just write text but actually return structured data like proper JSON objects with specific
[01:08:40] fields? That's where the structured output parser comes in. Uh think of it
[01:08:45] like giving the model a blue blueprint for for its answer. So instead of letting it respond however way it wants,
[01:08:51] we define a clear structure like for example we can say give me three facts about black holes and each fact should
[01:08:58] go into its own field. fact one, fact two and fact three. So we define this structure using response schema which
[01:09:05] defines uh what each field represents. Then the structured output parser uses
[01:09:11] that schema to make sure the AI's output follows the exact format we asked for.
[01:09:16] Uh usually that is a validation object. Now, if you worked with LLMs
[01:09:23] before or you followed our video, uh you know that sometimes the model does not
[01:09:28] exactly uh follow the instruction perfectly. So, it might it might miss a
[01:09:35] field, forget the brackets or add some extra text. Now, that's where the output fixing error or the output fixing parser
[01:09:43] saves the day. It automatically identifies when the output isn't in the correct format and uses the model itself
[01:09:50] to repair or reformat the response until it matches the required schema. So what
[01:09:56] we are trying to do here is we're really teaching the model to think structurally
[01:10:02] to not just like generate sentences but also to produce clean structured content
[01:10:08] uh that a program can directly read and use. So this actually becomes powerful when
[01:10:16] we want to build uh systems like knowledge extractors, facts, fact generators or even uh data analysis
[01:10:24] tools where consistency matters. And just like we did in the previous video,
[01:10:29] we wrap everything inside a chain. So the flow looks like the following here.
[01:10:34] So the prompt first defines the task, the model generate the response and the parser ensures the output is valid and
[01:10:42] well formatted. So the concept brings together everything we've learned so far. Clear prompting, structured control
[01:10:49] and automatic error correction, meeting making our AI output uh also ready for
[01:10:56] uh automation. So now let's go to the implementation of whatever we talked in
[01:11:02] this video.
[01:11:07] Let me create a new file called structured
[01:11:14] output parser.py. Okay, first model.
[01:11:25] We've been doing this since our first video. So I I hope you understand this.
[01:11:30] Then the credentials.
[01:11:40] Then we're importing a prompt template from langen core.
[01:11:52] Then we'll import our output parsers here. So uh it will again come from lang
[01:11:59] output parsers import structured output parser and a response schema.
[01:12:11] Okay I did something wrong here. So this is going to be
[01:12:18] oh okay so it does not come from langen port but it comes from langen. So output
[01:12:24] parsers import structured output parser and
[01:12:31] okay something is wrong here. Oh I should have written from I'm not
[01:12:36] import structured output parser and then a response schema.
[01:12:41] Now uh from langchain.output parsers I'll need output fixing parser to
[01:12:49] uh so this library allows the model to recorrect if there is any error in terms
[01:12:55] of it response. So load env I'll first define my model.
[01:13:15] Okay. Now we'll define our schema here or the response schema that we want.
[01:13:20] So basically we're going to pass a topic and then we will ask the model to
[01:13:26] generate three facts and then we'll create a placeholder using these response schema for those facts here. So
[01:13:33] so for the first one we'll call fact one and then
[01:13:39] uh it detail will be first fact about a
[01:13:44] certain topic. Okay we'll say black hole. So this will be our first one. We'll
[01:13:51] again define a response schema. We'll have fact two
[01:13:58] and then add a description. So this will be second fact about black
[01:14:04] hole. And then we'll do a response schema again. Name equals to
[01:14:13] fact three. description equals to third
[01:14:19] fact about black hole.
[01:14:25] Okay. So we will define our parser. So this is going to be a structured output
[01:14:30] parser but the structured output parser will be based on the schema that I have
[01:14:37] provided earlier. So structured output parser from response is schemas and then I'll and
[01:14:46] and then I'll pass in the schema here. So again at last if the model does not
[01:14:52] uh obey us in passing the in uh in generating the output based
[01:14:59] on this schema I'm going to use a output fixing parser
[01:15:04] from llm the llm is going to be our model and
[01:15:12] then the parser is going to be a parser which will strictly adhere the model uh
[01:15:18] to the given schema here. So now let me generate a template
[01:15:23] uh so template equals to I'm going to use this prompt template. We've already done
[01:15:29] this earlier. So template will be give me
[01:15:35] three facts about topic.
[01:15:42] Uh write something more. Return only valid
[01:15:49] JSON instruction
[01:15:55] instruction that follows this format and I'll provide a format here. Uh let me
[01:16:03] enclose this using string tag
[01:16:08] so that I can give multi-line strings here. So now uh the format
[01:16:16] will be uh format or the instruction format instruction
[01:16:24] or we'll we'll we'll call it the response format here. So response format
[01:16:32] uh the topic sorry the input variables at first is
[01:16:38] going to be our topic and then I'll also define a partial
[01:16:44] variables for the response format that we provided here. So response
[01:16:51] format and this is going to come from parser dot get format instruction.
[01:16:59] So this particular parser will provide this schema or the response format to this template here. Now I'll create a
[01:17:07] chain where I'll pass in the template first then a model first and then the
[01:17:13] passer here. So first of all the prompt will be generated using template then it will be
[01:17:20] passed to the model and then the model response will be fixed by the parser. Now I can invoke this chain
[01:17:28] using the entry point which is our template. So I'll need to provide a topic here
[01:17:36] and the topic will be black hole and then I can print out the result.
[01:17:44] Let me clear this and run this again.
[01:17:49] Okay, I missed topic here. So, clear this
[01:17:54] and run it again.
[01:18:01] There are a few spelling mistakes, but it's okay. I can see that I got a JSON format where I have fact one
[01:18:09] uh fact two and where three okay I have fact three just
[01:18:15] like the way that we've asked for in our schema map uh so what is being done here the prompt
[01:18:24] template is generating a prompt based on the template that we
[01:18:29] provided here the structured output parser uh is is asking the model to base
[01:18:36] its response on on the basis of this provider schema. Here the output fixing
[01:18:42] parser is strictly uh asking the model to follow this
[01:18:48] particular schema here in the prompt template. One extra variable we provided is the partial variables. that partial
[01:18:55] variables is the response schema that we provided here
[01:19:00] and then we've extracted it from the parser uh parser object of structured
[01:19:07] output parser and then we've generated a chain and generated the response. So I
[01:19:12] hope this runs for you on your end as well. If you in our previous video we talked about how large language models
[01:19:19] often gives us free flowing text responses and then how we can use tools
[01:19:24] like structured output parser to make those responses more organized and consistent. But now we're taking a same
[01:19:30] idea into a step further by introducing a much more powerful and professional tool called the pyantic output parser.
[01:19:37] So what exactly does it do? Now think of it in this way. So when we ask a large
[01:19:43] language model for structured information like someone's name, age or city, the model tries its best to follow
[01:19:50] our instruction. But sometimes it gives extra text, sometimes it forgets a field
[01:19:56] or sometimes it just formats things incorrectly. The parenting output parser
[01:20:01] uh helps us control and validate those responses. So it makes sure the models
[01:20:07] output isn't just structured but also it's correct, complete and reliable.
[01:20:13] This parser is built on top of a Python library called Pyantic. And Pyic is all
[01:20:18] about data validation. It actually lets us define what kind of data we expect.
[01:20:23] For example, like we can say the name should be text, age should be number and then it has to be greater than 18 and
[01:20:30] the city should be a word. So when the model gives us a response, the parser
[01:20:35] checks if everything fit those rules or not. If something is missing or invalid, u the parser immediately flags it or
[01:20:42] even corrects it when combined with a tools like out tools like output fixing
[01:20:47] output fixing parser. Now if you remember the structured output parser we used earlier in the previous video, it
[01:20:53] also gave us structured data. So what is the difference between these two here?
[01:20:59] So the key difference is that structured output parser mainly focuses on formatting. It helps the model respond
[01:21:04] in a defined structure but the pyetic output parser uh adds this layer of intelligence and validation on top of
[01:21:12] that. So it actually does not just organize the response but it also checks
[01:21:17] the response and ensures that it matches the exact data type and the constraints we define. So that means uh we'll get
[01:21:25] fewer errors uh cleaner data and a far more predictable output which is especially important when you're building real world application. So the
[01:21:32] beauty of this parser is that the once the model output passes through it, you
[01:21:37] can get a clean and reliable data ready to use in your code or application
[01:21:44] directly like you don't need to worry about converting things to string, fixing JSON
[01:21:50] or like checking for missing fields. Everything comes out neat and validated
[01:21:55] exactly the way you want it. Uh so the pyic output parser takes the concept of
[01:22:00] this structured output and then upgrades it with validation and reliability. If
[01:22:07] the structured output was our first step towards getting a structured response from LLM then the penting version is the
[01:22:14] professional grade tool uh the one that makes your AI pipeline more stable and
[01:22:19] then uh production ready. So that is why this concept is so much important to
[01:22:26] understand. Because as we start building more complex uh
[01:22:33] systems, we will rely on such kind of validated structured output to make sure
[01:22:38] our AI uh gives us the output in the format that we expect. So after this brief
[01:22:45] introduction, let's go to the code and see what we have talked about and implement it in real life.
[01:22:58] Okay. So I'm going to create a new file here. I'm going to call this pyantic
[01:23:05] parser.py. So I will have some import. First of all,
[01:23:12] our model
[01:23:19] then envy
[01:23:40] output parser.
[01:23:50] Uh you can also use this output fixing parser if you want but uh I'm not going to do this right now. I've already shown
[01:23:58] you how to use it in the previous video. So from Pyantic I'm going to import base
[01:24:03] model and then I'm going to import a field. Okay. So load the credentials
[01:24:10] define the model
[01:24:19] and after that I'm going to create this pentic validation class called person.
[01:24:26] So here I'll have base model as a parameter. I'm going to include three
[01:24:32] things name, age and string. So name is going to be a string. its field and its
[01:24:38] detail is uh the person's
[01:24:44] full name. Then I'm going to have an age. Age is going to be an integer and I'll also
[01:24:51] define its detail. So the age uh has to
[01:24:56] be greater than 18 and then it has to be less than uh let's say 100.
[01:25:05] Then I'll also add the detail or the description.
[01:25:13] The person's age must be greater than 18. Well, I've
[01:25:21] already defined that here. Now I'm going to define a city. It is going to be string.
[01:25:28] Okay. Field field field and its detail.
[01:25:34] So the city
[01:25:40] where the person lives. Okay. Now I have defined this class. I'm
[01:25:46] going to create an object a pyic parser
[01:25:51] pyic output parser and then the pyic object is the class that we have
[01:25:56] defined. Okay. Now let's create a template
[01:26:02] from the prompt template that we have. So the template is going to be I'm going
[01:26:08] to create a multiple string or multi-line string here.
[01:26:13] So the prompt template is giving me give me give me the name, age and city of a
[01:26:25] of a fictional fictional place.
[01:26:34] Uh okay. What else? Make sure the age is
[01:26:39] greater than 18 and less than 100. Also
[01:26:46] uh return return the response in following format
[01:26:53] and the format is our uh response format just like we did
[01:27:00] earlier. Okay. What else? I need a input variable. Input variable is going to be
[01:27:08] a place name and then my partial variables. Partial
[01:27:14] variables is going to come from uh this is response
[01:27:20] format and then it is going to come from parser or the parser object dot get
[01:27:25] format instructions. Okay.
[01:27:35] Now what I can do is I can directly define a chain. So it will first call up
[01:27:41] template then model and then the parser.
[01:27:46] Uh then after that I can invoke this chain
[01:27:52] using the entry point to this chain which is a place name. So let's say Nepal
[01:28:00] and then print a result here. So the code seems to be okay. If you
[01:28:07] have any problem viewing this, let me do a word so that you can see everything
[01:28:13] else here. Now let me run this code and see what we get in our response. We
[01:28:18] should get the name, age and city just like we asked for. Okay. So if if we see here we see a name Pracastahal age is 45
[01:28:27] and city is provided here just like in the format that we asked for. So this is
[01:28:33] how we use pyic output parser uh using lang graph. So this is all for this
[01:28:39] video. I hope it runs in your end as well. In our previous video, we've already worked with simple chains
[01:28:45] connecting multiple steps together where the output of one step becomes the input for the next. Now, we'll talk about a
[01:28:53] concept concept a bit deeper and talk about why chains are so useful and how
[01:29:00] we can make them more powerful using something called sequential chains. The biggest advantage of using chains is
[01:29:06] that uh they help us organize complex AI workflows into smaller logical steps. So
[01:29:12] instead of sending one massive prompt and hoping the models model handles everything correctly, we break the
[01:29:19] process down. Maybe first generate something then summarize it and then finally maybe analyze it. This modular
[01:29:26] structure makes our workflow much more cleaner, more reusable and then easier to debug. So if one part of the chain
[01:29:33] isn't doing well, we can just improve that step without touching the rest. So it's a very practical way to scale our
[01:29:40] AI projects. Now let's talk about sequential chains. Uh which is one of the most useful type of chains in lang
[01:29:46] chain. So sequential chains work in a step-by-step manner. The output from one stage is is automatically passed to the
[01:29:53] next stage in order. So think of it like an assembly line. So each component in the chain has a specific rule and
[01:30:00] together they create a complete workflow. So for example, step one could generate a detailed article. Step two
[01:30:07] could generate summarize it into a few sentences and then step three could
[01:30:12] maybe extract keywords or insights from that summary. The entire process runs very smoothly with data flowing
[01:30:18] automatically between steps. So now what's great about sequential chains is that they makes complex reasoning task
[01:30:25] easier to redesign and control. So we can exactly fix what happens at its its
[01:30:31] stage and then how information moves between them. And since everything is modular uh we can mix and match our
[01:30:38] components, change a prompt, switch a model or maybe add a new processing step all without rewriting the entire
[01:30:45] pipeline. So in this video we'll actually implement a sequential chain and see how it helps us combine multiple
[01:30:52] prompts and models into one connected process. And by the end of this video you will be able to understand not how
[01:30:59] not how just to build one or not just how to build one but also why sequential chaining is one of the most powerful
[01:31:05] ideas in line chain for creating intelligent and multi-step AI applications. So now let's go to the
[01:31:11] code. So I'm going to create a new folder here
[01:31:17] called uh it's folder number five called chains and inside this chain I'm going to
[01:31:23] create my first chain called sequential chain.py
[01:31:29] pipe. Okay. So again, first of all, we'll have
[01:31:34] a model
[01:31:39] and then avo
[01:31:46] template. So that is going to come from langen core.
[01:31:55] And then we'll also use this str output passer that we've already used earlier
[01:32:01] too. And then that is also going to come from langen core.
[01:32:08] Load our credentials. Define our model.
[01:32:19] Now I'm going to provide two template. The first template
[01:32:26] will be a prompt template. We've done a similar example earlier
[01:32:31] too, but uh I'm going to use it anyway here. So, generate
[01:32:37] three detailed uh detailed report
[01:32:43] on a topic topic. Uh I'm also going to use my input
[01:32:49] variables. My input variables will be a topic. So this is going to be my first
[01:32:54] prompt that will be generated from prompt template. The next will be
[01:33:00] another prompt that says uh generate a threepoint summary.
[01:33:08] Twoint summary on following text and then I'll provide a text here.
[01:33:15] Uh so my input variables is going to be a
[01:33:21] text. And then uh I already have a model. So
[01:33:26] I'll define my parser which will be from SR output parser. And then I'll define a
[01:33:32] chain which is a sequential chain here. So first of all I'll go to prompt one then I'll call the model then I'll go to
[01:33:40] parser and then I'll go to prompt two and then I'll again call a model and
[01:33:45] then I'll go to parser again and finally I I'll invoke this chain and then store the output in the result. So chain dot
[01:33:53] invoke I'll define an entry point for this chain which is topic. So,
[01:34:00] so my topic is going to be my topic is going to be uh 3II
[01:34:08] interstellar. Okay. 3i 3i atlas
[01:34:16] interstellar object which is quite uh in use these days. So I'm going to
[01:34:22] use this topic and then I'll just print out the result. So, so this here is a
[01:34:29] sequential chain here. So, let me run this and while this is being run, I'll try to explain you what's happening
[01:34:35] here. Okay, something's wrong.
[01:34:40] Ah, this is model name. So, let me run this again.
[01:34:48] So, we have our model. The first prompt is generated from this prompt template. The second prompt is generated from this
[01:34:55] prompt template. So based on the first prompt which is passed to the model and then and then the output is passed
[01:35:01] through the string output parser which means the entire output will be converted to string and then and then
[01:35:07] that particular output will be passed to this text here uh using prompt two and the model and then the output will be
[01:35:14] passed again by str output parser and then finally be printed here.
[01:35:19] So it might take some time to run because we've got multiple models here.
[01:35:24] So let's see the output.
[01:35:42] Okay, we have a three-point summary about 3i atlas. So the first point is
[01:35:48] provided here. The second point is here and the third point is here. The content is not that important right now. Uh I
[01:35:54] just wanted to show you the implementation of chains. In our last video we explored sequential chains
[01:36:01] where each step in the chain executes one after another in a fixed order. That was great for workflows that all that
[01:36:08] always follows the same path. But what if you want your AI workflow to make
[01:36:14] choices along the way? What if the next step depends on the output of the previous step? Now that's where the
[01:36:19] conditional chain comes in. So conditional chains lets you design workflows that branch based on certain
[01:36:26] conditions. So instead of a single fixed path, you can define multiple possible paths and the chain decides which one to
[01:36:33] follow depending on the model's response or some other criteria. So think of it like you choose your own adventure book.
[01:36:40] Depending on your choice, the story takes a different direction. Conditional chains give your AI workflows that same
[01:36:46] flexibility. Okay, let's take an example. So imagine you're processing customer feedback. So if the feedback
[01:36:52] mentions a bug, you might want to send it to the development team. If it's a feature request, you might want to send
[01:36:59] it to a product team. And if it's a compliment, maybe you log it and then thank the user automatically. So with a
[01:37:06] conditional chain, the workflow evaluates the content and routes it automatically. You don't have to
[01:37:12] manually check each case. The chain handles the branching logic. So the real advantage of a conditional chain is that
[01:37:19] they make your workflow dynamic and intelligent. So you're no longer limited to linear processes. Instead, your AI
[01:37:26] can adapt its upcoming step based on the data it sees. Now this is incredibly
[01:37:32] useful for real world application where inputs can vary widely like customer support, content analysis, data
[01:37:38] classification and much more. So in this video we'll implement a conditional
[01:37:44] chain and see exactly how to set up multiple parts and conditions. So by the end of this video, you'll understand how
[01:37:51] to create a flexible AI pipeline that responds intelligently to different situations, making your applications
[01:37:57] more robust and closer to human like decision-m. Now let's jump onto the
[01:38:02] code. Okay, I'll create a new file here
[01:38:11] and I'll call it uh conditional chain.py.
[01:38:19] First of all, we'll have a model.
[01:38:25] then envate.
[01:38:32] [Music]
[01:38:42] Then I will have uh the pentic output parser which will come from langent code dot output
[01:38:49] parsers import identic output parser.
[01:38:57] uh then I will use this runnable schema dot runnable
[01:39:04] importable branch for flexive branching chain logic and then
[01:39:11] runnable lambda for a lambda function since we using pyante model so we'll
[01:39:18] include a base model and a field and also uh let me write this
[01:39:28] literal class from the typing function. Load the library.
[01:39:36] Load the model.
[01:39:48] Okay. Now, uh I'll also have one more parcel which is the str output parser.
[01:39:55] So my parser object will also be created str out parser. Now I'll create a pentic
[01:40:02] class here for feedback. So this will be a base model.
[01:40:09] I will try to find the sentiment of the feedback which will have two options.
[01:40:15] And it can be the positive feedback or a negative feedback.
[01:40:20] And then its detail will be given here field and description
[01:40:26] is the sentiment of the feedback.
[01:40:34] Feedback provided. Okay. I will have a parser two which
[01:40:39] will be my pyic parser or pyic output parser
[01:40:45] and then the pyic object will be my feedback class.
[01:40:50] Now I'll have a prompt one
[01:40:57] which will be my prompt template uh
[01:41:04] prompt template.
[01:41:09] It template will be classify the classify the sentiment of
[01:41:19] following feedback text into positive or negative
[01:41:28] and then this will be my feedback test uh feedback placeholder. So input
[01:41:35] variables will be
[01:41:42] feedback and and I'll also have a partial variable
[01:41:48] uh because I'll provide a format instruction here uh statement and provide the response
[01:41:57] in following format and then this will have our response
[01:42:04] format. Right. So I'll have a partial variable. Uh
[01:42:11] uh. Okay. This has to be a dictionary. So response
[01:42:18] format is going to come from parser 2. Passer 2 dot get format instructions.
[01:42:26] Okay. I'm going to use a word so that uh it will be easier for you to view here.
[01:42:31] WordP. Okay. This is my first first prompt here. I'll first define a
[01:42:39] classifier chain which will follow prompt one
[01:42:45] model one or the same model that I have. I'll just use the model and then parser
[01:42:54] and then parser two because I'm going to use the pyic parser here. Next, I'll again have another prompt
[01:43:03] which is going to say template
[01:43:09] uh write an appropriate feedback
[01:43:15] to this positive sorry appropriate response
[01:43:22] response do this positive feedback
[01:43:27] and then I'll provide write my feedback here. I'll have an input variable.
[01:43:33] Uh this will be feedback and I also have prompt three
[01:43:41] that will work for the negative feedback.
[01:43:48] So negative feedback and then the value will be prompt three.
[01:43:56] So here I'm going to declare a branch chain.
[01:44:01] Branch chain as we can see this is a sequential chain we have here. But based on the output of this chain now we'll
[01:44:08] move on to the branch chain and then either select one of these uh two
[01:44:15] prompts and then generate the response. Now for branching I'm going to use runnable
[01:44:21] branch and not this bracket. I need a small bracket here. So runnable brands I'm
[01:44:27] going to declare a lambda function here. So lambda x x
[01:44:34] sentiment equals to
[01:44:40] uh positive. So, so if the sentiment is positive then
[01:44:50] uh my chain will be prompt to model and then parser
[01:44:59] parser else if my sentiment is negative lambda
[01:45:07] x x dot sentiment equals to
[01:45:14] negative then my chain will be from three
[01:45:20] from three model and then a passer
[01:45:28] and I need to declare this as a runnable lambda
[01:45:37] uh lambda x. So if else if and then the last one is else condition here.
[01:45:45] So if the sentiments are either of these positives and negative sentiment then I'm going to say no valid sentiment
[01:45:52] found in my review or feedback.
[01:46:00] Okay, the bracket should not close here. So this is done. Now what I'm going to do is I'm going to
[01:46:07] combine these two chains into one chain. So first one is our classified chain and then the second one is our branch chain.
[01:46:14] So we can also combine a sequential chain and a conditional chain into one
[01:46:21] single chain here like this. The first one that is going to be executed is our S is a sequential chain or the
[01:46:27] classified chain and and based on the response of that classifier chain we will then move to branch chain and then
[01:46:34] uh the model will work accordingly. So we'll invoke this chain dot invoke the
[01:46:41] entry point is the classifier chain and then and then in the classifier chain the entry point is feedback one sorry
[01:46:47] prompt one where I need to provide a feedback here. So let me provide a
[01:46:53] feedback and then say that the phone is actually
[01:47:03] actually amazing. Let me print the result and see what we
[01:47:08] get here. It should actually trigger prompt two and then we should get an
[01:47:15] appropriate response for this positive feedback here. So let's run this.
[01:47:21] So now we see that we get a positive response and then uh there are uh
[01:47:30] the the model classifies this feedback as positive and then based on it it
[01:47:36] generates a response. Now I'm going to change this. Okay, the phone is actually
[01:47:41] horrible. The UI is stuck and then things like that.
[01:47:47] The UI is stuck. And then if I run this
[01:47:58] so the model is in fact giving out a lot of things but you know that we can structure the output uh using the
[01:48:05] parendic output parser or this or or like we can base it on some some form of
[01:48:11] schema as we've already done earlier. So you can uh actually uh implement it
[01:48:17] yourself. So here we get that the response.
[01:48:24] Okay. So it says that when responding to negative feedback so it classifies the
[01:48:31] feedback as negative here for this thing. So in this video we saw how we
[01:48:37] could implement conditional chains using runnable branch runnable lambda as well as how we could combine a sequential
[01:48:44] chain and and a conditional chain into one and then make our uh workflow
[01:48:52] automated here. So far in the series we've explored sequential chains where each step executes one after the other
[01:48:59] and conditional chains where the workflow can branch based on certain conditions. Now we're moving into
[01:49:05] another powerful concepts in line chain called parallel chains. So what exactly is parallel chain? So in simple terms we
[01:49:12] can say that parallel chain allows us to run multiple workflows at the same time independently of each other and then
[01:49:18] combine their results. So think of it like a team working on various tasks
[01:49:23] simultaneously. So one team member is taking notes, another is taking or like creating quiz questions and and then
[01:49:30] later on someone merges both into a final report. So by doing things in parallel, we can save time, make the
[01:49:37] workflow more efficient and then handle multiple aspects of a task at once.
[01:49:43] And then that is also key advantage of parallel chain uh efficiency and
[01:49:48] modularity because each component can focus on a specific task without waiting for the
[01:49:54] others to finish and once all the parallel task are complete uh the output can be combined in a meaningful way
[01:50:00] creating a complete and comprehensive result. So what are we going to do in this video? So our plan for this video
[01:50:07] is first we'll take a block of text then we'll run two parallel task. One will
[01:50:14] generate short and simple notes from the text and then the other will generate a set of short quiz questions based on the
[01:50:21] same text and then finally once these two parallel tasks are done we'll merge the notes and the quiz into a single
[01:50:27] comprehensive document. This will show exactly how parallel chains can handle multiple output at
[01:50:33] once and then combine them seamlessly making our AI workflow both fast and
[01:50:38] organized. Now let's jump onto the code.
[01:50:45] So create a new file called sequent parallel chain not
[01:50:51] sequential chain parallel chain.py. Okay. First of all, our language model,
[01:51:02] then av function.
[01:51:08] Then I need a prompt template which will we'll get from langchen.prompts.
[01:51:20] Let's also use the str output parser.
[01:51:30] And we'll have a runnable parallel from langchen dots schema dot runnable for
[01:51:38] our runnable uh sorry for our parallel chain
[01:51:44] to run here. Okay. Now let's load the credentials. Define a model.
[01:52:02] Now first of all I'll define a prompt. First prompt will be
[01:52:08] about
[01:52:14] generating a short and like let's say generate short and simple
[01:52:22] uh nodes for or from the
[01:52:28] following for the following topic. Sorry. For the following
[01:52:34] topic, we would place a topic in the
[01:52:40] placeholder. Our input variables is going to be topic. We're not going to enforce any kind of
[01:52:48] schema for our response right now. So, we'll just leave it to this. Our next template will be a
[01:52:56] prompt template. And this will be about generating
[01:53:03] uh generate five short question answer from the
[01:53:11] following text or input variable is going to be a text
[01:53:22] and then after this I'll have prompt three prompt template it.
[01:53:29] Um this will generate uh
[01:53:36] okay short question answer is done and then the summary is also done right. So we will have prompt three to merge the
[01:53:44] provided notes and quiz into a single document.
[01:53:53] document uh then I'll provide nodes
[01:54:01] in nodes placeholder and then base in quiz placeholder.
[01:54:11] Okay. So now let's create an object of
[01:54:16] our parser here.
[01:54:23] So parser equals to strl output parser.
[01:54:30] I'm going to create a runnable chain parallel chain which will be done using
[01:54:38] runnable parallel. The first component of this chain is to create nodes and I'll name it nodes. How
[01:54:46] we'll run this is we'll first start with prompt one then pass it to the model
[01:54:53] and then uh model and then run a password on it.
[01:55:00] In the second chain we'll name it quiz. I will pass
[01:55:07] prompt prompt to prompt
[01:55:13] two then model and then a password.
[01:55:22] Okay. Now
[01:55:28] we'll call our final chain which will be prompt
[01:55:35] prompt three and then model and then passer.
[01:55:42] Now we'll combine the sequential chain here final chain and then the parallel chain runnable chain here we'll call it
[01:55:48] chains equals to first of all we'll execute our runnable chain and then we'll execute our final chain.
[01:55:57] We'll invoke this chain chain dot invoke
[01:56:04] and then we'll uh provide an entry point for this one. So the entry point to this
[01:56:09] chain has to be a text, right? So a topic or a text here. Okay,
[01:56:17] I'll just try to make it uniform. But it's okay even if we don't make it uniform. Uh because one is going to go
[01:56:25] in topic and then the other is going to go to text. I think uh I don't need to
[01:56:30] make it uniform, I guess. So let's write a text here. I'm going to copy
[01:56:35] this text from some someplace else. But I'm going to I will be using a word wrap
[01:56:41] so that you can copy uh copy this or like use any text that you want from
[01:56:46] anywhere. So this text is a support vector machines
[01:56:52] uh view word wrap. Okay, everything is wrapped inside this view. Now what I'm
[01:56:57] going to do is I'm going to invoke this simply invoke this text. The result is
[01:57:03] going to be stored here. Result equals to gen.invoke. And then finally I'm going to print my
[01:57:10] result. So this is my parallel chain implementation here. Let me run this
[01:57:16] code and then I'll explain what is happening here. Okay. While the code is running,
[01:57:22] let's go up. We have prompt one. Uh which generates a short and simple note
[01:57:28] about a following topic. Prompt two is like uh generating short answer question
[01:57:34] the following topic. And then prompt three is about merging the output from prompt one and prompt two. We have a
[01:57:42] parser and then I've I have initialized a runnable chain where we uh run these
[01:57:48] two chains in parallel at first. Then the output of these two chains are are then merged with the final chain and
[01:57:55] then uh a response is created. So if we see here so basically a complete note is
[01:58:01] created where we have support vector machine nodes. Uh the note is created
[01:58:07] here on top and then the quer is created here on the second and then both of these are combined into one
[01:58:13] comprehensible documents. So this is how we uh implement parallel chain in
[01:58:21] lang chains. So I hope this runs on your end too. If you have any questions or have any or if you have any queries,
[01:58:27] feel free to comment down below and I'll try to help you out. I will see you in the next video again.
[02:00:17] So now we see that we get a positive response and then uh there are uh
[02:00:25] the the model classifies this feedback as positive and then based on it it
[02:00:31] generates a response. Now I'm going to change this. Okay, the phone is actually
[02:00:36] horrible. The UI is stop and then things like that.
[02:00:42] The UI is stop and then if I run this.
[02:00:54] So the model is in fact giving out a lot of things. But you know that we can structure the output uh using the pyic
[02:01:01] output parser or this uh or or like we can base it on some some form of schema
[02:01:07] as we've already done earlier. So you can uh actually uh implement it
[02:01:13] yourself. So here we get that the response.
[02:01:20] Okay. So it says that when responding to negative feedback, so it classifies the
[02:01:26] feedback as negative here for this thing. So in this video we saw how we
[02:01:33] could implement conditional chains using runnable branch, runnable lambda as well as how we could combine a sequential
[02:01:39] chain and and a conditional chain into one and then make our uh workflow
[02:01:47] automated. Here we've explored different type of chain, sequential chain, conditional chains, and even parallel
[02:01:53] chains. Each of these help us design multi-step workflows and process information in a structured way. Now,
[02:01:59] let's focus on something simpler, the LLM chain. So, what exactly is an LLM
[02:02:05] chain? At its core, an LLM chain is just a singlestep chain. It connects a
[02:02:10] language model to a prompt template, letting us easily generate outputs from our LLM in a structured and reusable
[02:02:17] way. Think of it as a building block, a straightforward chain where we give a prompt, the model processes it and then
[02:02:23] gives an output. There's no branching, no parallel processing. It is just a
[02:02:28] simple clean uh workflow for a single task. The beauty of LM chain is in fact
[02:02:36] in simplicity and reusability. So for example, you might want the model to suggest a catchy blog title, summarize
[02:02:43] an article and generate ideas for social media task. So you create a prompt
[02:02:48] template for the task connect it to the model via the LLM chain and then you can get or you can reuse that chain for any
[02:02:56] input without rewriting your logic. So in this video here's what we'll do.
[02:03:02] We'll define a prompt template asking the model to suggest a catchy block title for a given topic. We'll connect
[02:03:07] it to a language model using LLM chain. Then we'll run the chain with a specific
[02:03:12] topic. And finally, we'll see how the chain returns a creative usable block title.
[02:03:19] So the following implementation is going to demonstrate how LLM chain provides a structured, repeatable, and simple
[02:03:25] workflow for generating AI outputs. So without any delay, let's go into the
[02:03:31] code. So I'm going to create a new file here.
[02:03:38] I'm going to name it LLM chains.py. As always, we'll first use our model.
[02:03:47] import chat Google generative AI from env import
[02:03:56] load.env from langchen_core.prompts.
[02:04:04] I'm going to import a prompt template
[02:04:09] and and the new concept today is from lang chain dot chains import lmchain.
[02:04:28] Okay. So first of all load the credentials. Uh
[02:04:36] we'll load a model Gemini 2.5
[02:04:44] flash. Then we'll create a prompt which will be used using prompt
[02:04:50] template. I'm going to write a template for this one. So suggest a catchy
[02:04:58] blog title about a topic and my input variable is going to be
[02:05:07] a topic. Okay.
[02:05:14] Now I'll I will define a chain which is going to be an LLM chain and
[02:05:21] I'm going to pass a model to this and then a prompt
[02:05:27] to this. Okay. and
[02:05:33] and then based on this let me write a topic name let's say 3 I atlas
[02:05:40] interstellar object
[02:05:46] I'm going to invoke this chain so response equals to chain do invoke I
[02:05:53] need to provide a topic here so topic is going to be my topic
[02:05:58] variable And then basically I'm going to print the response to this. So everything is
[02:06:06] done. Let me clear this. I'm not sure about this LLM chain. Uh it might be deprecated or something. Let's see.
[02:06:17] Okay. So this is deprecated uh with the method. Okay. Runnable
[02:06:23] sequence. So we've already done runnable sequence earlier, but it's okay. We're doing something that has already been
[02:06:28] deprecated. But still we get our answer here. Uh the topic is threei atlas
[02:06:34] interstellar object. Here are some catchy blog titles. Since we have not structured or directed our uh response
[02:06:42] to be something specific that's why the model is generating this long text here
[02:06:48] uh as a catchy block title which we don't don't in fact want but like uh it
[02:06:54] is what the model is uh providing me. So here's what you can do. You can uh use
[02:07:01] some form of response schema or maybe a parentic class uh to direct the model to
[02:07:08] provide just a catchy block title just a a combination of three to four words
[02:07:15] uh block title for this particular topic. So I'm going to end this video for now.
[02:07:22] uh if you have any problem or any queries do comment down below and I'll try to help you out and I'll see you in
[02:07:27] the next video. In our previous videos, we've worked with LMS and change giving prompts and generating responses and
[02:07:34] even combining multiple models to create structured workflows. Now we're moving into another fundamental concept that
[02:07:40] powers intelligent retrieval, search and reasoning in modern AI agents, embeddings.
[02:07:46] So what exactly are embeddings? Think of embedding as the numerical fingerprints of text. So when we feed a sentence like
[02:07:54] Delhi is the capital of India into an embedding model, it doesn't just read it as words. It converts that text into a
[02:08:01] vector which is a list of numbers. Each number in this vector represents a
[02:08:06] small piece of meaning from the original text. Together they form a semantic representation. meaning that two text
[02:08:12] with similar meaning will have vectors that are close to each other in this higher dimensional space.
[02:08:19] So we'll make this intuitive. Suppose if you take two sentences like Paris is the
[02:08:24] capital of France and Delhi is the capital of India. The embeddings will be very close because both talk about
[02:08:31] capital cities. But a sentence like I love pizza will have a completely different embedding far away in vector
[02:08:37] space. Now why do we need embeddings? Embeddings are basically essential
[02:08:43] whenever we want our AI to understand the context or meaning rather than just a plain text. They help us with semantic
[02:08:50] search meaning finding documents similar in meaning not just by matching keywords.
[02:08:56] It also helps in context retrieval that means fetching relevant chunk of
[02:09:03] information to feed on into an LLM. They help us with clustering and classification which means grouping
[02:09:09] related ideas together and they help us with recommendation system meaning suggesting similar product or content
[02:09:14] based on its meaning. So in this video we'll use hugging face
[02:09:20] embeddings specifically all the specifically the model
[02:09:27] in this video we'll use hugging face embeddings which is a lightweight and efficient
[02:09:34] in this video we'll use hugging face embeddings and a lightweight model from
[02:09:39] the hogging face embeddings so we'll generate embeddings for a single query that is the Delhi is the capital of
[02:09:45] India and then a list of related documents. Then when we print them, we'll see a long list of numbers and
[02:09:52] those numbers will be the numerical representation of our text. But what's more important is that we will use those
[02:09:59] emittings to compare similarities, store them in a vector database or build context aware AI agents that retrieve
[02:10:06] information intelligently. So main advantages of these embeddings are huge. First of all, they allow semantic
[02:10:13] understanding, meaning the model can reason about meaning rather than exact wording. Second, they make retrieval
[02:10:20] augmented generation possible or rag possible where an AI can look up context
[02:10:27] before answering. And the third, they significantly improve accuracy in search
[02:10:32] question answering and knowledge based systems. So embedding acts as a bridge between the language and numbers
[02:10:38] allowing machines to understand and compare the meaning of provided text or document. And as we go
[02:10:46] further in this series, we'll see how embedding power intelligent agents enabling them to remember, reason, and
[02:10:53] respond more accurately. But for now, we'll see the very basic implementation
[02:10:58] of these embeddings. So let's go into the code.
[02:11:07] Okay, I'm going to create a new folder here and then call it embeddings.
[02:11:18] Uh after that I'm going to create a new file. Let's call it hugging face
[02:11:23] embeddings py.
[02:11:32] uh hing face embeddings do pry. Okay. Now the import is going to
[02:11:38] be quite different here. So first of all I think I'll need to install something.
[02:11:43] Let's go to requirements here. I'll need to install lang chain
[02:11:50] auging fishing face. Okay. I'm going to install this
[02:11:55] these requirements as as everything else is installed. The new library will also
[02:12:01] be installed in addition to the other libraries that we already have.
[02:12:09] Okay, this is installed. Let's go back to our code and then from here we'll import from
[02:12:16] langchain_hog import
[02:12:21] hugging face embeddings then we'll re uh import
[02:12:29] uh load envoy import OS I'm not sure if this is going
[02:12:34] to be used or not but anyway let's import it and I'll load my credential okay So what kind of credential am I
[02:12:40] loading here? So to use hugging face embeddings you need to visit the hugging
[02:12:46] face website and create your API key and then save it in the environment file
[02:12:51] here. So you can do that yourself or take the reference from the hoggingface
[02:12:57] documentation too or the lenins documentation and then uh you can create
[02:13:02] that create that hogging API key and then paste it in your env and and then
[02:13:08] that will be loaded here using this load env function. Anyway, I'm not going to go to that. I've already placed my
[02:13:14] hogging face embedding key here. So I'm directly going to run this. So embeddings equals to hogging face
[02:13:20] embeddings. And then I'm going to use a model here that I found out in the hogging face model section. So this
[02:13:28] model is sentence transformers all
[02:13:34] mini LM L6 V2.
[02:13:40] Uh I I might also need to import sentence transformers here.
[02:13:47] So let me write that down. And then uh
[02:13:52] install the requirements install minus r requirements. txt.
[02:14:03] So while that is being installed, let's continue with our code. As I said, I'll have a text,
[02:14:09] simple text. Delhi is the capital of India.
[02:14:14] Uh, and along with this, I'll have some documents
[02:14:21] where I'll say Delhi is the capital of India.
[02:14:26] Uh, Kolkata is the capital of West Bengal.
[02:14:36] and then Paris is the capital of France.
[02:14:45] So what we can do here is like I said we can convert this textual data into
[02:14:50] numerical figures. So to do that I'm going to use embedding model dot embed
[02:14:56] query embed query and then inside this I'm
[02:15:01] going to pass this text here. So if you print this out ste result if you print
[02:15:08] this out then you'll see that this particular text will be converted to
[02:15:14] numbers okay so let me run this now the library is now installed I can run this
[02:15:19] code now it will take some time to run because the model might not be downloaded so it
[02:15:25] will download the model first and then uh run our program here okay so once I
[02:15:32] printed did this and the code has run. We can see that this particular text is
[02:15:38] now converted into into this combination of numbers here. Now this is called
[02:15:44] embeddings. Not only this, we can also embed our documents here. So if I just type
[02:15:56] So if I just type result doc equals to embedding dot since we embedding
[02:16:04] documents so we need to pass in embed documents and then pass in the documents in this list. Now if we print this uh
[02:16:13] we'll see that we have uh embeddings for each of these text in a document. So we
[02:16:20] basically get a list of embeddings and then each embedding will
[02:16:26] resemble each sentence that we see here. So I think you can run this program on your end and then see the result of your
[02:16:33] embedding here. uh what we what we'll also do is we'll also try to compare the
[02:16:40] similarity of this particular text with these documents here. So let's see what happens. Uh what we'll do is we'll do
[02:16:49] result here and then result do here.
[02:16:54] Now we'll compare the similarity score and
[02:17:00] and then for the similarity score we'll use cosine similarity. For that we'll need to uh import something here or
[02:17:06] basically need to install a library install pip install scikitlearn
[02:17:14] and then based on that particular uh scikitlearn library
[02:17:24] we'll import from skarn.mmetrix
[02:17:29] dot dotpise we'll import cosine simarity Okay.
[02:17:34] And now to compare our uh similarity embeddings between our text
[02:17:41] and our document, we'll use cos and similarity. And we'll pass uh
[02:17:49] our result here. A result which is the embedding of our
[02:17:55] text. And then we'll pass our document embeddings which is result_doc.
[02:18:02] So let me print out the similarity scores here. So let me simply print out similarity
[02:18:09] scores whatever we get here
[02:18:14] and then run this program again. So as you can see uh the text
[02:18:22] is uh the similarity between the text and then and then the first document is
[02:18:29] almost 98%. Similarity between the text and the second document is almost 47%
[02:18:35] and the third and the third document is all is 27%. Now if I add a a new
[02:18:42] sentence like uh I love pizza here we'll see that this similarity score is quite
[02:18:49] low in comparison to the above three sentences. Let's print it out. Let's clear this and then run this model
[02:18:56] again. So we can see that the text Delhi is capital of India is
[02:19:05] very much lower similar to I love pizza which almost 9% similar to pizza. So the
[02:19:11] highest highest similarity is these two sentences. You might get why did not why we did not get 100% similarity because
[02:19:19] here uh there is a slight mistake while I've typed this word. So if I type Delhi
[02:19:25] is the capital of India and then we get the same sentence here it would basically be 100%. Let's see if if we
[02:19:32] get a 100% similarity between them or not. So yeah we get a 100% similarity
[02:19:38] score between these two sentences. So this is the importance of uh embeddings
[02:19:44] basically converting the text into numbers represent the semantic meaning of that particular text uh which is
[02:19:51] easier for computer to understand and everything from now on that we'll do
[02:19:56] will be based on these embeddings. Basically if embeddings were not possible uh retrieval augmented
[02:20:03] generation would not have been uh possible. In the last video, we explored
[02:20:09] the concept of embeddings. How they transform text into numerical representation that capture meaning.
[02:20:15] Now, we're taking one step further and actually use those embeddings to make our model retrieve
[02:20:21] and reason over real data. So, in this video, we'll build a simple retrieval augmented generation or a rack pipeline.
[02:20:29] a system where the model doesn't just rely on its internal memory but instead looks up relevant information from
[02:20:36] external sources before giving an answer. So we'll break down step by
[02:20:42] step. The first thing we do here is load a document using a text loader class. So
[02:20:47] it's a straightforward way to bring in external data. Uh for example, a text
[02:20:52] file, a web page, or even a PDF. Here uh we're going to use a txt file. uh which
[02:20:59] might contains note research summaries or articles. Next, we use the recursive
[02:21:05] character text splitter. Now, this is a very clever tool. Uh what it does is it
[02:21:11] breaks large text into smaller manageable chunks, usually around 500 characters each in our case.
[02:21:18] But why do we do that? Uh we do that because language models and embeddings
[02:21:24] work much better when the text is short and focused. So by so by uh giving them the byite
[02:21:33] side chunk with side overlap we have the model preserve the context while
[02:21:39] avoiding cut off issues especially useful when dealing with large documental books. Now once the text is
[02:21:45] split we move to the embedding step. So here we're going to use uh Google's
[02:21:51] Gemini embedding model. Each of these chunks will be now converted into a
[02:21:57] vector or embedding just like we saw saw in our last video. Now these embeddings need to be stored
[02:22:03] somewhere efficiently and that's where f comes in. So f uh stands for Facebook aim Facebook a similarity search uh and
[02:22:11] it is a powerful open source vector database that allows us to store and search documents very quickly.
[02:22:19] Uh so using F what we do is uh each uh
[02:22:25] each chunk of our content is embedded into a vector and then those vectors are stored in a f index and f prepares
[02:22:33] itself to find the most similar vectors whenever we search. Then uh we'll go for
[02:22:39] something called a retriever. So what happens in a retriever is it is like a
[02:22:44] search engine for our AI agent. So when we provide a query for example like what are the key takeaways for our documents
[02:22:50] the retriever searches through all those embeddings inside files and then returns the most semantically similar search. So
[02:22:58] this step ensures that our model doesn't try to remember everything. Instead what it does is it retrieves the right
[02:23:04] information on demand. Uh so after that we combine all of those retrieve text
[02:23:11] chunks into a single string of context. This context acts like the background material or nodes that we pass to our
[02:23:17] language model. Then finally we initialize our GI chart model and manually construct a prompt that
[02:23:23] combines both the context and the question. So when we pass this prompt to the LM uh it uses the provided context
[02:23:30] to generate a factual context of their answer and then not something made up from its internal knowledge. So this
[02:23:37] workflow is is the backbone of what's known as retrieval augmented generation or rack. Instead of letting the model
[02:23:44] guess or holen it uh we ground it with reality data here so the benefits are uh
[02:23:51] huge the model gets more accurate and reliable. Uh it
[02:23:57] can also handle custom data sources and it's far more scalable since we can swap documents add new data and then do
[02:24:03] something like so in this video we've taken we will take a big step from embeddings
[02:24:09] to retrieval. Now uh let's go to the code.
[02:24:16] So I'm going to create a new folder here
[02:24:23] and call it rag or basic rag for now.
[02:24:29] Let a new file basic rag.py
[02:24:35] what I'm going to do is I'm going to create a docs.txt txt file
[02:24:41] and then uh bring uh bring some paragraphs of content. Uh here I already
[02:24:47] have it. So I'll have this content about AI. I'm going to do word. You can bring
[02:24:53] anything else that you want here. It's fine. So yeah, this is my uh document for now.
[02:25:03] Okay, we'll go back to our basic RA model. So I'm going to generate my model
[02:25:09] first
[02:25:14] and then env
[02:25:20] uh I'm also going to load a text loader from langen community.
[02:25:33] Then I'll have a texter from Langen.
[02:25:46] Then uh let's also import our embeddings from
[02:25:53] Google geni. Then we'll use vector stores
[02:26:07] to store our embeddings here and then we'll use a retrieval QA. So from lang
[02:26:14] chain of chains
[02:26:20] put retrieval uh not this one
[02:26:28] retrieval QA okay now based on this first first let me
[02:26:34] load the credentials load the credentials okay I'm providing stepwise
[02:26:43] uh information here. So step one, I'm going to load my credentials. In step two, I'm going to load the document that
[02:26:52] I have. So I'll be defining a loader and then using this
[02:26:58] class here called text loader. My file name is docs.txt
[02:27:04] and then I'm going to load this. So documents equals to loader.load.
[02:27:10] In step three, I'm going to split the text into smaller chunks.
[02:27:17] Smaller chunks. So for this, I'm going to use a text splitter.
[02:27:23] Recursive text. Sorry, recursive character text splitter. Uh I'm going to define the chunk size of each
[02:27:31] uh splitted text. Let me give 500. and
[02:27:36] then also chunk overlap because uh
[02:27:42] we might carry some contextual information in while breaking those chunks. So we want some overlap so that
[02:27:50] uh two different chunks can have something common something in common between them. So I'm going to split my
[02:27:57] document using the splitter. So text splitter dotsplit documents
[02:28:03] documents
[02:28:09] Okay, now I'm now I'm going to
[02:28:15] Okay, this is step three. I'm going to step four where I'm going
[02:28:20] to convert text embeddings and store in.
[02:28:29] So here what I'm going to do is I'm going to define embeddings equals to
[02:28:36] Google gen Google generative AI embeddings where the model is model/
[02:28:43] Gemini embedding uh 001 I found this in the langen
[02:28:50] documentation itself and then I'm going to define a vector store using phase dot from documents
[02:29:01] docs Sorry from documents docs comma.
[02:29:10] Then in step five I'm going to create
[02:29:16] create a retriever which fetches
[02:29:22] relevant documents.
[02:29:30] Okay. retriever equals to vector store dot as
[02:29:36] retriever and then I'm going to initialize a model.
[02:29:46] Sub model equal to chat Google generative AI model equals to Gemini 2.5
[02:29:59] you can even use hoging based models here step seven
[02:30:05] I'm going to create a retrieval QA chain
[02:30:12] so chain equals to retrieval QA dot from_chain
[02:30:19] type where llm equals to llm and
[02:30:24] the retriever equals to retriever
[02:30:30] or model here then uh
[02:30:37] I'm going to manually query manually query
[02:30:46] uh model and retrieve and relevant
[02:30:54] documents. So query is going to be like okay what is the document about? Let's see.
[02:31:01] So it is about AI. So my query will be very simple. So what are the key
[02:31:08] takeaways from the document?
[02:31:15] And then based on this query I can invoke my chain here. So chain dot
[02:31:22] invoke query. And then finally I'm going to print my answer.
[02:31:34] So print response. Okay the code is done. I have
[02:31:39] tried to provide you step-wise implementation of what we talked about in the rack pipeline. So let me run
[02:31:46] this. Let me also save this document and then run this here.
[02:31:57] So the model will provide answer based on okay I need to something I need to
[02:32:03] install something here. So 5G CPU needs to be installed. Let's write it down in requirements.txt. txt2
[02:32:16] GPU install this with install minus r requt
[02:32:23] txt.
[02:32:29] Okay, we can install the CPU version because my laptop not be equipped with
[02:32:36] GPU here.
[02:32:43] Okay. So, let me run this once again.
[02:32:59] And while this is being run uh the answer from the LLM is based on the documents
[02:33:07] that are provided and not on its internal knowledge base. So the key so the key takeaways are uh
[02:33:15] are like actually provided here uh if we need to ask spec something specific from this document like uh what is this? So
[02:33:25] doing such tasks such as learning reason AI comprise okay what kind of technology that AI comprise let's ask that
[02:33:39] so what kind of technology technologies
[02:33:45] does AI comprise of? So if you ask this question
[02:33:51] then the model is going to provide the answer
[02:33:56] based on the document and not its internal knowledge.
[02:34:04] So it says uh AI comprise a variety of technology it's copied just from this particular context here. So this is how
[02:34:11] we build a basic direct pipeline. I hope you understood the concept. We've been exploring how different types of chains
[02:34:17] help us connect multiple components together from simple prompt to model pipelines to more advanced parallel and
[02:34:24] conditional chains. But now we're going one step further and introducing a concept that gives us even more control
[02:34:31] and flexibility in how our data flows through a chain. Runnable sequence. So
[02:34:37] what exactly is a runnable sequence? So think of it like a conveyor built for data. Each step in the sequence takes
[02:34:44] the output from the previous step, processes it and passes it along to the next one. It's part of the new runnable
[02:34:52] interface in lang chain which unifies how different components like prompts, models and parsers communicate with each
[02:34:59] other. Earlier when we worked with LLM chain, the structure was was quite
[02:35:04] fixed. One prompt, one model, one output. It worked great for single
[02:35:10] single or simple workflows. But when we wanted to do something more dynamic like
[02:35:15] taking a model's output, processing it through another template, and then feeding it back to another model, things
[02:35:22] start to get messy. That's where a runnable sequence shines. It lets us
[02:35:27] build multi-step pipelines where each element can be a model, a parser, or even another chain all connected in a
[02:35:35] single streamlined sequence. You can think of it as a customizable chain where you fix exactly how data flows
[02:35:43] from one step to another. Now, in this video, we'll be doing something fun to demonstrate it. We'll start with a
[02:35:49] prompt that asks the model to write a joke about a given topic. Then, we'll take that generated joke and then feed
[02:35:55] it to a second prompt, one that asks the model to explain the joke. And then finally, we'll use an output parcel to
[02:36:02] cleanly process and present our final response. So by the end of this video
[02:36:07] you'll understand how runnable sequence works, how it is different from earlier
[02:36:14] chain structures we've seen and how you can use it to build smooth multi-step reasoning pipelines where the model's
[02:36:20] output becomes the input for the next stage. All right, so let's dive into the code.
[02:36:36] So I'm going to create a folder here called Runnables
[02:36:48] and then uh file called runnable
[02:36:53] sequence.py. Okay. So first of all import lang Google
[02:37:00] genai then I need av
[02:37:09] after that I'm going to import prompt template which will come from langen core.
[02:37:22] Then I will import an STR output parser that is also going to come from lang gen
[02:37:27] code
[02:37:40] from line 4 from line 4 dot output passers import
[02:37:51] output passer And after that I'll need a runnable sequence which will come from
[02:37:56] langen dots schema dot runnables import
[02:38:02] runnable sequence. Okay. So first of all load the
[02:38:08] credentials. I'll define my first prompt where
[02:38:14] using my prompt template and then the template is going to be about write a
[02:38:19] joke about about a topic and inside this my input
[02:38:29] variables is going to be topic.
[02:38:35] Similarly write second prompt again using a prompt template where my
[02:38:41] template will be explain the following
[02:38:47] joke and then I'll type in joke here my input
[02:38:54] variables is going to be
[02:39:00] joke. Okay. So my model will be from chat
[02:39:08] Google AI. The model will be Gemini
[02:39:16] 2.5 flash.
[02:39:21] I'll also define my output parser using str output parser
[02:39:27] and then based on that I'm going to define a chain or a runnable sequence.
[02:39:32] So first of all I'm going to go to prompt then I'll go to model then parser
[02:39:39] then prompt two then model and then parser again
[02:39:47] uh and then result I'll just invoke this runnable sequence
[02:39:53] book. So the topic is going to be a monkey with like and
[02:40:01] then and then finally print a result here. So this is how you implement runnable sequence. You might see that
[02:40:09] this is similar to the concept of chains that we used. We if we had used chains
[02:40:15] we would do it in this way. So change it would be prompt
[02:40:20] model uh parser then prompt two
[02:40:26] and model and then parser again. So this is how we
[02:40:32] would implement chain but here we're doing it with runnable sequence. Let me run this code.
[02:40:54] It will take some time because we have multiple model calls here. One one model call is this part and then
[02:41:00] the other model call is this part and then without the completion of the first model call uh we cannot go to the second
[02:41:08] model down here. Okay. So couldn't feel the love anymore. So the humor breaks
[02:41:13] in. Okay, it's only printing me out the explanation about the joke. But uh but
[02:41:20] it's okay. Uh uh it's okay because uh I've got what I wanted at the end of
[02:41:26] this runnable sequence. In the last video, we explored runnable sequence where each step in the chain passed its
[02:41:33] output to the next creating a smooth step-by-step flow of data. But now we're flipping that idea around and diving
[02:41:39] into something that works side by side instead of one after another, which is runnable parallel. Simply put, runnable
[02:41:46] parallel allows us to run multiple sequences or tasks at the same time in
[02:41:51] parallel. While runnable sequence is all about order and dependency where where
[02:41:56] step two awaits for step one, runnable parallel is about independence and speed. Each branch runs simultaneously
[02:42:03] on the same input producing multiple outputs at once. So imagine you're a content creator and you have one topic,
[02:42:10] say AI, and you want a tweet, a LinkedIn post or maybe an Instagram caption about
[02:42:16] it. So instead of generating them one by one, Runnable Parallel lets you generate
[02:42:21] all of them in a single call. So that's exactly what we'll be doing in this video. We'll create two separate
[02:42:27] prompts. one to generate a tweet and another to generate LinkedIn post about the same topic. Then we'll wrap them
[02:42:34] inside a runnable parallel. So both prompts are executed at the same time using the same model. The final output
[02:42:42] will give us both results neatly, one as a tweet and then the other as a LinkedIn
[02:42:48] post. So the real advantage here of runnable parallel is efficiency. When you're building complex AI workflows
[02:42:54] like summarizing document, generating multiple content format or running
[02:43:00] multiperspective analysis, runnable parallel helps you process everything faster and more efficiently uh without
[02:43:07] waiting for one task to finish before starting the next. So in this video we'll see runnable parallel in action,
[02:43:13] understand how it complements runnable sequence and also explore how combining both can help us build truly powerful
[02:43:22] multi-step multi-output pipelines. So let's get started and then go to the code.
[02:43:33] So I'm going to create a new file called runnable parallel
[02:43:42] pie. Uh I'll be using the same import. So I'm just going to copy it from here. Okay.
[02:43:49] So these three imports are the same. Uh these four inputs are the same. And then I will need to uh call up runnable
[02:43:56] parallel and runnable sequences. So langen dot schema dot runable
[02:44:03] import runnable uh sequence and then a runnable
[02:44:10] runnable parallel. Okay. So this is also same uh I'll have prop one which tells
[02:44:19] me or which is about
[02:44:24] generating a tweet. So generate
[02:44:29] a tweet about topic
[02:44:34] and then the input variables here is a topic
[02:44:40] and another prompt is also the same. So I'm just going to copy this. So prompt two is generate a
[02:44:49] LinkedIn post about a topic. So this is going to be prompt two.
[02:44:57] I'll define my model. So model equals to
[02:45:02] representative AI model equal to GI
[02:45:08] 2.5
[02:45:13] have a parser. So parser equals to str output parser
[02:45:23] and after that I'm going to implement a parallel chain using uh runnable parallel. So
[02:45:31] a r parallel chain equals to runnable
[02:45:36] parallel and inside this I'll have two different runnable sequences.
[02:45:46] Okay. So the first one will be named tw and inside this I'll have a runnable
[02:45:52] sequence.
[02:45:59] uh tweet it is going to be a runnable sequence
[02:46:06] where I'm going to pass in prompt one model and then passer and similarly
[02:46:14] I'll also have second runnable sequence
[02:46:20] it will be about two model and parser
[02:46:27] now uh going to invoke this par chain par chain.invoke
[02:46:34] uh the topic is going to be uh
[02:46:40] we will do runnable parallel in lang
[02:46:46] and then I'm going to print the result. So this is how we implement a runnable
[02:46:52] parallel in lang. uh we'll need uh chronable sequences for individual
[02:46:59] parallel events here. So let me run this and then see the output. Again it is going to take some
[02:47:06] time to run because I have multiple runnable sequences here. So these two need to be implemented but they'll be
[02:47:12] implemented simultaneously at once and then I'll get the final output here.
[02:47:24] Uh also if we need to maybe bind our output in some for some some form of
[02:47:30] schema we know that we've already done that. Uh we can do it using pyandic or
[02:47:36] maybe uh maybe by defining some for some form of uh schema and then we have done
[02:47:42] it in our earlier videos. If you haven't seen that uh you can go back and then
[02:47:48] check it out and try it yourself. So here we have a tweet and then down here we'll we also somewhere have a LinkedIn
[02:47:54] post uh that we need to find but at least it's there somewhere uh it is
[02:48:00] there somewhere around there and then uh in in case of LinkedIn post I think I'm getting some code to code to here like
[02:48:08] these these lines I think are the codes here so anyway we have implemented a
[02:48:14] runnable parallel uh in this particular program. Today we're diving into one of
[02:48:21] the most essential building block of any langen project which is document loaders. So if you've ever wondered how
[02:48:28] we bring data from the real world like PDFs, text files, websites or CSVs into
[02:48:33] our AI pipeline, this is where it all begins. So document loaders actually act
[02:48:38] as a bridge between raw data and intelligence system. So what exactly are
[02:48:44] document loaders here? So think of them as a specialized tool that read and import data from different sources into
[02:48:50] langen standardized format called documents. So each document contains not just text but also
[02:48:57] metadata things like file name, source, URL or page number which later helps us
[02:49:03] organize and query data more efficiently. In simple terms, they are
[02:49:09] the data injection engines of blanken. And today we'll look at five most uh
[02:49:15] five of the most commonly used ones. Text loader, pipe PDF loader, directory loader, web based loader, and then CSV
[02:49:22] loader. So first of all, we'll start with the text loader. As the name suggests, this one deals with plain text
[02:49:28] files. So it's perfect when you have .txt documents, maybe transcript logs or notes that you want to feed into your
[02:49:35] system. where uh it it opens the file,
[02:49:40] reads the text and wraps it into a length and document object which is quite lightweight, fast and ideal for
[02:49:47] structured plain text data. The next is a PIP PDF loader. one of the most
[02:49:52] popular loaders especially in editable based AI system because PDFs are
[02:49:58] everywhere in research paper, invoices, reports, books and they often carry a
[02:50:04] lot of valuable information. So, PIP PF loader extracts the text from each page
[02:50:09] and converts them into a structured documents keeping track of the metadata
[02:50:15] like page numbers and file sources. And then it makes it incredibly useful when
[02:50:21] you need to query or summarize specific sections of a PDF rather than treating it as a single long text. Now, what if
[02:50:29] you have hundreds of files sitting inside a folder and that's where document loader comes in. instead of
[02:50:34] manually loading each file uh sorry directly loader. So directly loader
[02:50:40] scans through your folder and automatically loads every document that matches your design pattern. For
[02:50:45] example, all all txt or PDF files. It is perfect for large scale projects like
[02:50:51] when you're building a chatbot trained on a company's entire set of initial docu or internal documents. So it also
[02:50:59] saves you from a lot of manual effort and ensures consistency across all your data sources. Now uh this is where
[02:51:07] things get interesting because not all data lives on your local machine. Sometimes the information you need uh is
[02:51:14] scattered across web pages, blogs or online articles on the internet and then that's where the webbased loader comes
[02:51:21] in. uh this loader fetches the data directly from a web page URL, extracts the text content uh cleans it up and
[02:51:29] then turns it into a usable document. So it's extremely handy for web scraping or
[02:51:34] live data retrieval. For example, pulling FAQs from a website, news content from an article or product info
[02:51:42] from an online store. With web based article, you're not just limited to static files. uh your AI can now learn
[02:51:50] from live evolving web content from the internet. And finally, we have a CSV loader. So that's the one built for
[02:51:58] structural table data like spreadsheet data sets or analytical reports. Instead
[02:52:04] of reading the file as raw text, CSV loader processes each row of your CSV file as a separate document allowing the
[02:52:11] model to understand and retrieve uh structured information easily.
[02:52:17] especially useful when combined with retrieval or Q&A uh components where you
[02:52:23] might want your AI to answer questions like which product has the highest
[02:52:29] highest sales last month or like who's the top uh growing employee and and then
[02:52:37] all sorts of stuffs like that from a CSV based data source. Uh so as we have
[02:52:44] completed uh these five these five document loaders so each loader solves a
[02:52:50] unique problem here. First text loader helps with plain text. PI
[02:52:57] PDF loader handles structured PDF documents. Directory loader manages bul data. Webpage loader brings in dynamic
[02:53:04] online content and CSV loader handles structured text. Now we're going to see
[02:53:09] the implementation of each of these loader in our code.
[02:53:14] Okay. So first of all I'm going to create a folder uh what is it number nine and I call it
[02:53:21] document loaders and then we'll start with the first one
[02:53:27] which is our text loader.py.
[02:53:32] Okay. I'm going to uh create a few files here. Let's uh
[02:53:39] okay for this we'll use the docs uh docs txt file that we already have and then
[02:53:46] we'll load this. Let me word so that you can see the content of this what it's
[02:53:52] written here. I'm not going to use the model uh I'm just going to use the loader and then show you by loading the
[02:53:59] documents. So from line chain community dot document loaders import
[02:54:07] text loader. Okay. Uh now you need to create a loader
[02:54:16] object from this text loader. So text loader and then provide a file name
[02:54:21] cricket uh sorry docs.txt. And then if you need to provide an
[02:54:28] encoding uh you you can also do it. So let's do it. Encoding UTF8
[02:54:38] and then once you do this you can print out your docs here. Um so first of all
[02:54:45] let's do loader do.load and then you can print out your docs
[02:54:50] here. Okay. Uh what did I do here? Let's exit this and then run this again.
[02:55:00] So as you can see the content inside this docs folder has been loaded in
[02:55:08] loaded from my file here and then the source is docs.txt. What you can also do
[02:55:14] is you can also see the type of this docs. So if you print the type of this docs
[02:55:21] then you'll see that the type is list here in in the
[02:55:26] list you have uh document which encodes the metadata that encodes source uh and
[02:55:33] then inside the document we also have page content here. So you can directly also use this. So print docs dot sorry
[02:55:43] docs zero dot page page content which will directly print
[02:55:51] you the content inside the text that we have or you can also print out the
[02:55:56] metadata doc0 dot metadata and then you'll be able to see
[02:56:03] the meta which is the source that we have. Okay. So this is
[02:56:10] our first loader that we have.
[02:56:16] Now we'll move to our second loader which is our PI PDF loader.
[02:56:23] New file. So PIP PDF
[02:56:29] loader.py. Here I'm going to copy a simple PDF file
[02:56:36] from somewhere uh and then paste it here. Okay, you can grab any PDF file
[02:56:41] that you want. uh and I'll go to pipdf
[02:56:47] loader and I'll import from langen
[02:56:52] community document loaders import
[02:56:58] by pdf loader. Now
[02:57:04] initialize the loader object see
[02:57:09] and my file name is uh dlc curriculum PDF. So I'm going to use that same file
[02:57:15] name PDF
[02:57:20] just docs equals to uh loader dot load and then print out the
[02:57:28] doc. So whatever is present in this document will be loaded and then printed out.
[02:57:41] Okay. So we need to install PIP PDF. So let's do this.
[02:57:47] PIP PDF pipf.
[02:57:53] Okay. and then uh type install. We'll
[02:57:59] just do pi pig. Everything is else is installed here.
[02:58:05] And now that is installed, we can run our program again which is pip pdf loader.
[02:58:11] Where is that?
[02:58:20] And you can see our PDF page content has been loaded and it is loaded in the same
[02:58:26] format which contains a a list and then that contains a document. Document contains some metadata inside this and
[02:58:34] then along with that it it also has its page content. So you can follow the same
[02:58:39] process as as we did earlier here to print out the page content as well as the metadata. Okay. Now we'll move to
[02:58:48] the third type and then our third type is going to be our directory loader. So
[02:58:54] for that I'm I'm going to uh
[02:59:00] copy some PDFs inside a directory. So I basically have these AI uh AI books and
[02:59:08] a financial report. uh and then using
[02:59:14] using this PDF inside this directory I'm going to load the content of this particular books folder here. So let me
[02:59:21] open a new file. I'm going to call it directory loader.py.
[02:59:28] Uh you can choose any any kind of PDF inside a folder and then that will work.
[02:59:34] Okay. So from langen community dot document loaders
[02:59:41] import uh directory loaders and then we'll also do pi pdf loader
[02:59:48] because we have pdf content inside this.
[02:59:54] So loader equal to directory loader
[02:59:59] directory loader and then we'll provide a path to this. So path is a folder
[03:00:04] called books and then inside that I have uh contents
[03:00:11] like a file name dot PDF
[03:00:16] and then I'll also define a loader class and my and my loader class is going to be a pi PDF loader. Okay. So
[03:00:25] what I can do is I can do uh loader dot lazy load
[03:00:35] and then for document
[03:00:40] in docs we'll only print the meta data here. So
[03:00:49] document dot meta data. Okay, let me clear this out and then run this again.
[03:00:56] So, I have four different uh PDFs here and
[03:01:02] and for those four different PDFs, my document loader has has started
[03:01:08] loading whatever it can find inside it. So since there is a lot of metadata that
[03:01:15] I have, so it has been loading uh every metadata it can find inside my
[03:01:25] uh folder here. So it's loading each and every page.
[03:01:30] That is why the metadata uh print is is going on. But I think you understand
[03:01:36] what we tried to do here. So I'm going to do terminate this.
[03:01:42] And now we'll move to our fourth one or our fourth loader
[03:01:51] which will be our webbased loader. So let me create a new file
[03:01:58] web base loader.py. I'm I'm going to import webbased loader
[03:02:05] from langen community dot document loaders import
[03:02:11] the web base loader. Okay, now I don't need anything else. I'm
[03:02:18] simply going to copy a URL about uh
[03:02:25] about a car review that I could find on the internet.
[03:02:30] So from this I'll use loader
[03:02:36] equals to web based loader and then I'm going to pass the URL docs equals to loader.load
[03:02:44] and then I'll print out the docs here. So this will open a website that
[03:02:50] contains a car review uh on it and then I'm loading I'm trying to load the text
[03:02:57] present in this particular URL here. So let me run this.
[03:03:02] Okay. So it also needs beautiful soup because it will it will I think go for
[03:03:08] web scripping of this. So reinstall PS4 or beautiful for here.
[03:03:21] I'll also mention it on the requirements.py. Okay. So since this is
[03:03:27] installed so let me go and then run web based loader again we clear this
[03:03:34] and run this it might take some time because scraping
[03:03:40] might take some time here and then you can see that all of the text inside this are loaded and then it's again in the
[03:03:47] same format so so the format is uniform it has a list inside it it contains a
[03:03:54] document inside it we we metadata and then there are a lot of
[03:03:59] other contents in the metadata as well and then the page content actually contains the text content of that
[03:04:05] particular website. Now finally uh I I'm going to uh work on the last last type
[03:04:14] of loader which is a CSV loader. So for that I'm going to paste a CSV file here
[03:04:21] on my on my project folder. So you can see the CSV file that I have.
[03:04:27] It's just a comma separated file that contains some data. You can use any kind of CSV file that you want. And now I'm
[03:04:34] going to go to document loader. I'm going to create a new file. Five CSV uh
[03:04:40] CSV loader.py. Okay. Now again the import is the same
[03:04:45] from langen community. We import document loaders. uh import
[03:04:52] CSV loader and then the loader is going to be an object of CSV loader. Inside
[03:04:58] this I'm going to provide a file path. File path is going to be what is my file
[03:05:04] name. So social network ads dot CSV. Let me rename this and then write data dot CSV so that it'll be easier for me to
[03:05:12] write down the file name here. So data dot CSV and data equals to loader do.load upload
[03:05:20] and then if I print data here I will be able to see my CSV file content in this
[03:05:27] let me run this and you can see it's again pre being presented in a similar
[03:05:33] format inside the list there is a document and there is a metadata then inside it it contains source and page
[03:05:39] number here uh sorry page content here so this is how how how we use uh these
[03:05:46] five document loaders this is an essential part of our retrieval augmented generation system
[03:05:53] because uh on the very first step we need to load uh we need to ingest our we
[03:06:00] need to load our data uh into our rack system which can be used as a knowledge
[03:06:06] base for our rack project. So this is how we use it. I hope you understood the
[03:06:13] concept and also the implementation works for you. So imagine you're building an AI assistant that needs to
[03:06:19] remember everything you ever taught it from long documents, research papers, or even notes scattered across multiple
[03:06:26] files. Now, you wouldn't want your assistant to go through all those documents every single time you ask a
[03:06:32] question, right? It would be like asking a librarian to read every book in the library before giving an answer. And
[03:06:39] that's where vector store in Langen comes into play. They're like the memory banks of AI system. So far in this
[03:06:45] langen journey, we've talked about how to load data and how to split it into smaller manageable pieces. We've also
[03:06:51] seen how to generate embeddings, those numerical fingerprints that represent meaning rather than just words. But once
[03:06:58] we have all those embeddings, we need a place to store them efficiently and a way to search through them
[03:07:04] intelligently. And that's what a vector store does. You can think of a vector
[03:07:09] store as a smart semantic database. Instead of storing plain text, it store those highdimensional numbers vectors uh
[03:07:17] that capture the meaning behind the text. So when a new query comes in, the system converts it into an embedding too
[03:07:24] and then compares it with the stored vectors to find the ones that are most similar in the meaning. So even if your
[03:07:30] query doesn't use the same word as your data, it can still find the right answer because it understands what you meant,
[03:07:36] not just what you said. So for example, if you ask who is known as Captain Cool,
[03:07:42] a traditional keyword search might not help unless that exact phrase exists somewhere. But a vector store contains
[03:07:49] or the vector store understands that Captain Cool is semantically related to
[03:07:54] maybe Mahindra Singh Dhoni because their embeddings are close to each other in vector space. That's the
[03:08:01] real power of vector database. delete your AI think in terms of meaning and not in terms of matching words. So in
[03:08:08] langen we can use several vector stores like f chroma or pine cone among others.
[03:08:13] Uh in our case in this video we using chroma which is lightweight and works perfectly for local experiments. It
[03:08:20] stores all those embeddings persistently. So even when you restart your application your AI doesn't forget
[03:08:26] what it has learned. And when you connect this to your retrieval pipeline everything falls into place beautifully.
[03:08:32] The documents get embedded stored in the vector database and when a user asks asks a question the system fetches the
[03:08:39] most relevant chunks uh not because of matching keywords but because of the shared meaning here. So those retrieved
[03:08:44] chunks are then passed to your language model as additional context helping it generate accurate and informed answers.
[03:08:51] So vector stores are one of the key reasons uh modern AI systems feel so intelligent. They make your assistant
[03:08:58] capable of remembering, reasoning and retrieving information just like a human
[03:09:04] instantly recalling the most meaningful pieces of information it has seen before and without vector store your AI would
[03:09:10] simply be guessing and with them it's reasoning based on the memory.
[03:09:16] So in short uh we can say that vector stores turn static knowledge into searchable understanding giving your AI
[03:09:22] a memory that's fast, scalable and deeply semantic. So now let's go for the
[03:09:27] implementation of search vector store.
[03:09:33] Okay. So I'm going to create a new folder again here. Let's call this folder 11.
[03:09:42] Call it vector store. Okay. Inside this I'm going to create my first file.
[03:09:50] Vector store.py. So uh let me use the langen hogging face
[03:09:56] embeddings. So for this part so langen
[03:10:03] hoging face embeddings let's also import chroma uh which is our vector database that
[03:10:09] we're going to use. So I don't think I've imported this. So let me install pip install
[03:10:17] lang chain chroma
[03:10:25] and I'm also going to mention it on the requirements. So lang chroma
[03:10:40] from langchain chroma import chroma
[03:10:47] uh once it is installed uh the error will then go okay chroma then we'll also
[03:10:57] import document So from langen dot schema import
[03:11:03] document. Okay. Uh I'm also going to load my load my
[03:11:11] hogging face credentials here. So from env import
[03:11:16] load.env and load env function is called.
[03:11:23] Let me define my embedding model here. So I'm going to copy this
[03:11:29] uh from my previous code. So this model will be used as hoging pace embedding. I
[03:11:35] think we've already used this in our previous code too. So this is not new for you. So here I'm going to create
[03:11:41] some some document. Let me just copy these uh these are too longs for me to
[03:11:47] type. So I have some document related to some IPL players.
[03:11:52] So let me just copy this and then use word. You can pause the video and then see what is being typed here. So now
[03:12:00] document one contains page content related to Virat Kohli and his team
[03:12:05] called Royal Challenge Bangalore. Document two is about Sharma and his team. Document three is about Ms. Tonyi
[03:12:12] and his team. Document four is about Jaspumra and his team. And then document five is about Rabinda Jarda and his
[03:12:17] team. So these are the documents that I need to store in my vector embeddings. Uh so here what I'm going to do is I'm
[03:12:25] going to combine all of this in a single list. So doc 1, do 2, do 3, do 4 and do
[03:12:33] 5. Okay. Now let me initialize my vector store. This will come from chroma here.
[03:12:41] Chroma. And inside this I need to define my embedding function.
[03:12:46] My embedding function will be my embedding model that I've defined. my pers directory
[03:12:55] let's just me name it chromat a new folder called chromad will be created in my project and then my embeddings will
[03:13:01] be stored inside that then after that I'll need a collection name
[03:13:08] so the collection name will be sample
[03:13:22] Now since my vector store is initialized and now I'm going to add my documents to the vector store. So vector store dot
[03:13:28] add documents and then I'm going to add the collection of document that I've created here.
[03:13:35] Uh-huh. So what else? Let me retrieve vector
[03:13:41] store.get and then uh let me give some in
[03:13:50] including words here. So embedding should be included whenever I retrieve from vector store. Document should be
[03:13:55] retrieved whenever I fetch from vector store and then metadatas should be retrieved. Okay. Now
[03:14:04] I will have my query here.
[03:14:09] Let's say uh who among these are
[03:14:14] ballers. So I have some players here or cricket players among them. My query is who
[03:14:21] among these are ballers. So what I'm going to do is I'm going to run a vector search. So vector store dot run a
[03:14:30] similarity search. My query will be my query here and then
[03:14:36] I will fetch a top three result from this particular query.
[03:14:41] So and then store everything in result and then let's print out our result
[03:14:47] here. So let me run this. uh
[03:14:57] my documents will be embedded using this embedding model here and then be stored in this vector database uh and from that
[03:15:04] vector database uh I will simply run a query and then run a similarity search
[03:15:10] in that query and then get the relevant result here. So let me see uh the first
[03:15:17] one Mumbai Indian Jas Bumra is a baller that's nice. Uh the second one Mumbai
[03:15:25] Indian Rohit Sharma is also shown as a baller and then and then the
[03:15:32] third one Robinda is also shown as the baller. So Virat Kohli and then MSI are
[03:15:39] excluded from these search list and then based on the similarity it's found that V Rohit Sharma Bumra and then Robin Aar
[03:15:48] are ballers here. Let me also run uh another query here if it's okay. Uh
[03:15:57] we can also do something like this. So we can also print our result based on
[03:16:03] similarity search with scores. So if we see this then then we can we should see
[03:16:10] that the result rohit sharma should have a very low similarity score here. So
[03:16:15] similarity search with score. Okay. Uh our query will be our query
[03:16:24] and then and then we'll have a top three result here.
[03:16:30] So let me run this again and then we should see that the similarity score for Rohit Sharma should be quite less than
[03:16:37] uh Bumra or Ravinday because Rohit Sharma is not in fact a
[03:16:44] baller. Sometimes he does bowl off spin uh but not much. So let's see
[03:16:52] uh play for okay the first one is the spig bumra. So this is shown.
[03:16:59] Okay. Wait. Okay. I've not printed the result. Let me also print the result. Let me omit omit the previous result.
[03:17:05] And then let me print it here. And then let me com comment this out.
[03:17:12] And I just want to see the result. The syntactic score.
[03:17:20] And then you might be wondering where this chroma DV is. It should be well within here sometime. So yes, you can
[03:17:28] see the folder called chromadv and then uh that and then using that particular
[03:17:35] uh chrom vector database my result is being pulled here. So just with bumra has a similarity of something like 1.01.
[03:17:46] So again it's saying just with bumra and then it's saying just with bumra again. So, so the top result is
[03:17:54] top result is only Bumra and then not the other bowlers here. So, this is how
[03:18:00] uh vector store works. It converts our data
[03:18:05] or documents into embeddings and then store it and then based on the query we have provided we can fetch
[03:18:13] uh following contents as well as uh what we can do is we can uh get the most
[03:18:21] similar result or like run the similarity search on our given query
[03:18:27] against the stored documents. So I hope you understood the concept for vector store. It is just like a just like a
[03:18:33] database but instead of storing your data it stores the embeddings of your documents. You've loaded a ton of
[03:18:40] documents onto your langen pipeline. PDFs, text file, web pages, you name it. But now comes the big question. How do
[03:18:47] you exactly find the most relevant piece of information when the user asks asks a question? And that's where the retrieval
[03:18:54] comes into play. Retrievers are like the memory search engine of your engine system. They don't generate new
[03:19:01] information. Instead, they fetch relevant documents from your existing knowledge base. So whenever a user query
[03:19:07] comes in, the retriever looks at all your index data and returns only the
[03:19:12] chunks that are most relevant to that query. In simple terms, think of retrievers as your AI AI's librarian.
[03:19:20] You ask a question and it doesn't hand you the entire library. it quickly find the exact pages or paragraphs that
[03:19:27] matters the most. Now, under the hood, retrievers often work with vector stores. Whenever you add documents, they
[03:19:34] converted into vector embeddings, numerical representation that capture the meaning of the text. Then, when a
[03:19:40] question comes in, it's also converted into a vector and the retriever compares it with all stored vectors to find the
[03:19:46] closest matches. This is how your system ensures semantic relevance. It's not just matching words but
[03:19:53] understanding the meaning. Langen offers various types of retrievers. Some are
[03:19:59] simple keyword based ones, others are embedding based and then others are even specialized ones like multiquery
[03:20:06] retrieval or contextual compression retriever that enhances the retrieval quality using alms. Retrieverals are uh
[03:20:15] essentially be or retrievers. The retrievers are essential because they
[03:20:20] bridge the gap between your static document and your conventional AI. They ensure your LLM has contextually rich
[03:20:26] relevant data to reason with without needing to retrain or fine-tune the model itself. And in this video, we'll
[03:20:33] get hands-on with one of the most commonly used retrievers, the Wikipedia retriever, where we'll see how to pull
[03:20:40] realtime factual information straight from Wikipedia to enrich our AI response. So now let's go to the video.
[03:20:51] Okay. So, I'm going to create a new folder again here. Uh, I'll call it Okay. What is the
[03:21:00] size? Okay. 12. So, let's me create a folder 12 dot
[03:21:06] retrievers. Uh, I'll create a new file inside this. Let's call it Wikipedia
[03:21:14] 3r.py. Okay. So from langen community
[03:21:21] retrievers import wikipedia retriever uh
[03:21:27] then I'm going to define my retriever object here. So retriever object r
[03:21:33] equals to wikipedia retriever. I'm going to get the top key results and
[03:21:39] then the language I can also say the language and then the language has to be English. So my query is uh let's say
[03:21:48] Indian Premier League and then uh what I can do is I can use
[03:21:56] my ret object to invoke this query.
[03:22:03] Uh I'm not sure how many documents I'll find but let's print out print out the
[03:22:09] length of the document that I'm going to find here. Select the docs. Let me run
[03:22:14] this. Okay. So I need to install Wikipedia. So
[03:22:19] let's go to our requirement. Add a new library. Save it. I'm directly going to install
[03:22:26] this without going to the requirements.py or other requirements.xt. So let me
[03:22:34] rerun this again. Uh where is that? Wikb.
[03:22:39] So let me run this.
[03:22:46] It will take some time because it will again uh go to go to Wikipedia and then
[03:22:52] uh get the results from there. So yeah, this is my
[03:22:58] uh since I'm since I've only asked for top two results. So it is giving me two
[03:23:03] results and then for this one I can print each each and every of my results. So for I do in innumerate
[03:23:13] uh docs I can print is the result. So f uh
[03:23:20] result I + 1 and then uh print content
[03:23:29] and inside content I can print uh
[03:23:34] doc dot page content. Okay. So if I print this again I will
[03:23:43] have my two documents as well as its content too. And then that is straight from some
[03:23:49] Wikipedia pages about this particular topic here. So this is how we use
[03:23:55] retrievers. Uh in the next video we'll see we'll see something else uh
[03:24:02] or like we'll see uh more more in the retriever section and then we'll also use it on the local data or local
[03:24:10] documents. In the last video we explored what retrievers are and why they are such an essential part of langen. Now
[03:24:16] let's see how they actually come to life with the help of something called a vector store retriever. Think of a ve
[03:24:22] store as a smart database that doesn't just store text it stores the meaning. We've already talked about this. Every
[03:24:28] sentence, paragraph or document we feed into it is converted into a numerical representation called an embedding. And
[03:24:35] then these embeddings capture the semantic meaning of our text. So instead of searching for exact words, retriever
[03:24:41] searches for similar ideas. So in this video we're using Chroma which we've already used earlier which is one of the
[03:24:47] most popular open source vector databases out there. Uh
[03:24:53] so once we add our documents into Chroma it handles all the heavy lifting storing indexing and efficiently searching
[03:24:59] through our vetoriiz data. Now to generate these embeddings we'll be using hogging face embeddings a special model
[03:25:06] that we've already used in the past. uh the model will transform each document
[03:25:12] into a highdimensional vector allowing Chroma to understand the relationship between different pieces of text and
[03:25:18] then when we convert our chroma vector store into a retriever uh what we're really doing is giving our application
[03:25:25] to application the power to search semantically. So when you ask a question
[03:25:30] like what is chroma used for uh it doesn't just look for that exact phrase
[03:25:35] but in but it's fine sentences that means something similar even though they're using different words. So the
[03:25:42] combination of hogging face embeddings and chroma retriever forms the backbone of the retrieval augmented generation
[03:25:47] rack system that we've been talking about and then a simple form of rag we've already implemented in a previous
[03:25:53] video. So by the end of this video you'll see how this retriever transforms ordinary text into a scalable
[03:25:59] intelligent knowledge base that your AI can reason over effortlessly. Now without any delay let's go to the code.
[03:26:10] Uh so I'm going to delete this chroma database for now because uh it already
[03:26:16] contains embedding uh previous embedding. So I'm going to create a new file here inside retrievers and then
[03:26:23] call it vector store retrievers.py.
[03:26:28] So let me import chroma first. So from langchen
[03:26:35] chroma import chroma from langchen_h hoging face
[03:26:42] import hogging face embeddings from langen_core dot documents
[03:26:49] import document okay so uh I'll also need to load env so
[03:26:57] from env import load env
[03:27:03] let me into load env load the hogging face credentials here I'm going to create a document I'll just copy paste
[03:27:10] it uh from somewhere so these will be my source documents here let me wrap this
[03:27:17] up okay so these will my source documents here we have uh these four
[03:27:23] these four contents uh each defined inside a document
[03:27:29] Then uh we'll initialize our embedding embedding model.
[03:27:35] So this will be our hogging face embedding model. We'll use sentence transformers and then a model inside
[03:27:41] this sentence transformer. So what we're going to do is we're going to create a vector store.
[03:27:47] We've already done this again. So we'll use chroma dot from documents
[03:27:56] from underscore documents. Uh
[03:28:02] so our documents will be the document that we have here. Our
[03:28:08] embedding will be the embedding model that we have
[03:28:13] and then the collection name. collection name let's say will be the sample. Okay. Now we'll convert our
[03:28:20] vector store into a retriever. So the retriever equals to vector store dot has
[03:28:28] retriever. I just need to pass the search keyword arguments. Uh search
[03:28:36] quirks equals to uh let me pass a dict here. So a equal to two. So I'm so I'm
[03:28:44] basically getting a top two result here. So query equals to let's ask about uh
[03:28:50] embedding. So what does embedding do
[03:28:56] and then uh based on this I'm going to invoke my result. So retriever
[03:29:04] dot invoke uh and then I'm going to get query
[03:29:10] and based on this I'm I'm just going to print the result whatever it throws at
[03:29:15] me. So now uh let's run this. We've we have used
[03:29:22] vector store uh as a retriever here. Let's run this and then see what
[03:29:28] happens.
[03:29:36] So this all of these are being com converted to embeddings and then a chromb uh
[03:29:44] database is been created and along with that it is being converted to ve uh retrievers uh it is
[03:29:52] being converted as a retriever and then giving me the results. If you see here what I can see is
[03:30:01] whenever I I ask about embedding it gives me this
[03:30:06] result at first and then the embedding result. Do I have a embedding result? No. I have
[03:30:13] this chroma uh and then also I have this langen result here. So I have these
[03:30:20] three documents provided to me for this particular question here or let me
[03:30:27] change my question and then ask what is comma used for and I just need one
[03:30:35] result here. Let me venture it to one and run this.
[03:30:52] So again as you can see my search directly redirects me to this chroma is
[03:30:59] a vector database optimizer LM search. It also provides me some more answer of
[03:31:04] uh few more answers here related to embeddings and then the openi but but my
[03:31:11] most relevant uh result is the particular document here or this
[03:31:18] particular document here for the query here. Okay. So this was another type of
[03:31:25] retriever that we've used. In this video, we're going to bring together everything we've learned so far, from
[03:31:31] document loaders and text splitters to embeddings, vector stores, and retrievers, and use them to build
[03:31:38] something real. Imagine being able to ask questions directly about the content
[03:31:44] of any YouTube video without manually watching or scrubbing through it. That's exactly what we're going to do. We'll
[03:31:52] start by pulling the transcript of a YouTube video using the YouTube transcript API. This gives us all the
[03:31:58] spoken text which then becomes the foundation of our knowledge base. Now
[03:32:03] we'll use a recursive character text reader to break that long trans transcript into manageable chunks so our
[03:32:10] system can process and retrieve information efficiently. Then comes the magic. We'll convert those chunks into
[03:32:17] embeddings using a hogging face model and store them in a fires vector database. This allows our system to
[03:32:24] search and retrieve context content that's semantically related to any questions we ask. Once that's done,
[03:32:32] we'll turn fires into a retriever, which will find the most relevant parts of the transcript related to our query.
[03:32:40] Finally, we'll use a Gemini flash model, Google's powerful language model to
[03:32:45] analyze that content and generate an accurate transcriptbased answer to our question. In short, we're transforming a
[03:32:52] YouTube video into an intelligent, searchable knowledge source. By the end of this video, you'll understand how all
[03:32:59] the individual engine components like loaders, splitters, embeddings, retrievers, and models come together to
[03:33:06] create a fully functional question answering pipeline powered by a real video content. So, let's go to the
[03:33:12] implementation.
[03:33:18] So, I'm going to create a new folder again.
[03:33:25] Let's call this rack systems
[03:33:38] and inside this I'm going to create a new file YouTube_rank.py.
[03:33:45] Okay, first of all, I'm going to uh pull up YouTube transcript API
[03:33:54] import YouTube transcript
[03:34:00] API and transcripts
[03:34:06] disabled. We'll need to install this library here because I once I try to run this.
[03:34:14] So, we'll need to install YouTube transcript API. So, let me just do pip
[03:34:20] install YouTube transcript
[03:34:25] API.
[03:34:32] Okay, then I'm going to import
[03:34:39] recursive character text here. After this
[03:34:46] uh embedding from our hogging face models
[03:34:53] then we'll import our model.
[03:35:03] I'll also import import the prompt template.
[03:35:13] Then I'll import the vector database.
[03:35:22] And then finally to load our credential I'll import load.b.
[03:35:33] So step one is indexing or document induction.
[03:35:40] documentation. So I'll take a video ID and for video ID
[03:35:47] I'm going to take my own YouTube video here somewhere. Uh let me also load this credential and let's go to the video ID
[03:35:55] part. Okay. So so here is uh our video on on like
[03:36:03] building a rag pipeline which we did uh couple of days earlier in uh video
[03:36:09] number 17. So I'm just going to copy this particular video ID from from the
[03:36:14] URL and then paste it in my code.
[03:36:22] Okay. So this is going to be my video ID.
[03:36:28] Then what I'm going to do is I'm going to try uh
[03:36:34] and then I'm going to initialize an object of YouTube API or YouTube transcript API
[03:36:41] and then I'm going to pull my transcript here. So, y API dot
[03:36:50] fetch. My video ID will be my video ID and then
[03:36:57] uh the language of the video is English. I want transcript in English.
[03:37:03] Then I'm going to flatten the transcript
[03:37:09] to plain text. So for this what I'm going to do is
[03:37:15] transcript equals to dot join
[03:37:21] chunk dot text for chunk
[03:37:26] in transcript list. We'll also put an exceptions here. So if
[03:37:33] transcripts are disabled then we'll say
[03:37:41] no captions available for the video.
[03:37:48] Okay. So this is step one. We'll move to step two.
[03:37:54] In step two we'll do indexing uh which means our text splitting.
[03:38:01] So for that we already have our splitter here. So let me initialize the object of the splitter. So splitter equals to
[03:38:09] recursive character text splitter. I'm going to define a chunk size of maybe 1,000 and then a chunk overlap. Okay,
[03:38:17] we'll not do 1 th00and 1,000 is too long. So we'll do a chunk size of 300 and then a chunk overlap of 50.
[03:38:29] And then I'm going to use the splitter to split my text here or split my transcript. So create documents
[03:38:38] transcript if you want to see how many chunks we've
[03:38:45] broken it down into. So let's see if we can indeed uh extract the captions of
[03:38:51] transcript from our video or not. So let me run this.
[03:39:02] Okay, so we have 33 chunks. We can uh we could extract the extract the transcript
[03:39:08] from our video. So let's go to step three. The step three is embedding
[03:39:15] generation and storing in vector database.
[03:39:25] Okay. So, embedding model will be used from hugging base embeddings and then the model name is
[03:39:36] uh sentence
[03:39:43] transformers all mini LM L6.
[03:39:51] V2 uh the model I found it from our
[03:39:57] hogging face library. Uh let's also initialize our vector store. So vector
[03:40:03] store is done using files. So from documents
[03:40:12] chunks and then embedding equals to embedding model. Okay. Next is retrieval
[03:40:22] retriever. Uh so I'm going to create my retriever from
[03:40:28] my vector store as uh as a retriever
[03:40:34] and then the search type is going to be based on similarity
[03:40:39] and then uh search keyword arguments. Uh let me do this
[03:40:46] again. So search key load arguments
[03:40:51] is equal to uh the top let's say results here.
[03:41:01] Okay. Let me design my prompt using my prompt template.
[03:41:11] So in case of prompt template let me define my template here.
[03:41:17] So let's provide you are a helpful
[03:41:28] assistant answer only from the
[03:41:34] provided transcript context. If the context is
[03:41:43] insufficient just answer I don't know and then uh
[03:41:54] let me add a line break and then mention context
[03:42:00] is equal to context and uh question
[03:42:07] another line break question is equal to question.
[03:42:15] Okay. So this will be my template here. Uh
[03:42:28] I don't think I need to put an string. So let me remove this. So input variables are context and questions.
[03:42:38] and text and question. Okay, so this is done.
[03:42:44] Now let me deise my question here. Uh what is the video
[03:42:52] talking about and then we'll also join a contain
[03:42:59] context text. So context text is going to be joined from uh let's do this / m
[03:43:08] dot join dot dot page content for
[03:43:17] for doc in uh retrieved docs and then
[03:43:25] retrieve docs is going to The
[03:43:33] retrieve doc is going to be retriever dot invoke question.
[03:43:42] Okay. So after that I'm going to create my final prompt.
[03:43:50] So final prompt equals to prompt dot invoke. I
[03:43:58] need to pass two variables. One is the context which I can provide from context
[03:44:04] text and then the other is the question which I can provide from my question here.
[03:44:14] And finally I can invoke my model. Uh so model dot invoke
[03:44:21] final prompt I've defined my model right okay I've
[03:44:26] not defined my model so let me also define my model here so model equal to chat Google generative AI
[03:44:35] model equals to Gemini 2.5 model and then
[03:44:43] answer dot content from here so let Let me run this and then see if we can
[03:44:49] actually get the answer from our generated transcript or not.
[03:44:56] Let me remove this print statement for now and then run it.
[03:45:03] Uh I missed a comma. Let's see. Yeah, I should have put a comma here
[03:45:09] at the end of the text. Let me run it again.
[03:45:22] It will take some time for transcript generation as well as uh the embedding model loading and also uh it will take
[03:45:30] some time uh for the retriever to work
[03:45:35] here. So at first the model has been downloaded. I think I've already used this model
[03:45:41] earlier. I might have changed something here. Let me check. Okay. So,
[03:45:48] yeah. Okay. So, model was uh downloaded. So, the video is talking about using
[03:45:54] embeddings to make a model retrieve and reason over real data. It will build a simple table generation. Okay, it works.
[03:46:01] Uh in fact, we're talking about the f model in that particular video. Uh and then it works. So, I'll pass something
[03:46:09] that is not present in the context. So I'll pass this question here. Uh my
[03:46:15] question will be so does the video or like
[03:46:23] what does what does the creator say
[03:46:30] creator say about Brazil in the
[03:46:37] world cup football? I don't think the context for this is present in the video. So the model
[03:46:44] should say that it simply doesn't know about it. Let me run this again.
[03:46:59] It would be efficient uh to just uh store this uh transcript locally once it
[03:47:06] is downloaded. that every time um the transcript fetching part is not done as
[03:47:12] well as the vector store can also be stored. So as you can see uh for this
[03:47:17] question what does the creator say about Brazil in the World Cup football? It simply says I don't know because that particular context is not present in our
[03:47:24] transcript. So this is it. So we have successfully implemented a YouTube
[03:47:30] transcript uh knowledge base and then from that created a retrieval augmented generation system uh answering our
[03:47:37] particular question. So I hope uh it runs on your end too. In the earlier
[03:47:42] videos we've used language models in a very traditional way at chatbased systems. You'd give them a prompt, they
[03:47:48] would generate a response and that was it. But now things are evolving. The new wave of this agentic AI isn't just about
[03:47:55] generating text. It's about actions. Models are no longer limited to just answering questions. They can now call
[03:48:02] functions, interact with external systems, and even perform real operations on the behalf of user. This
[03:48:08] is where tools in Langen comes into play. Tools act as a bridge between the reasoning power of an LLM and the
[03:48:14] functional capabilities of your code. Instead of the model telling you what to do, it can now do it by invoking the
[03:48:21] right tool. Imagine this. You ask an AI, what is the product of five and 9? So
[03:48:27] instead of reasoning through the multiplication itself, it recognizes that there's already a defined tool that
[03:48:33] can perform this exact task and call it automatically. This shift is massive
[03:48:38] because it transforms LLM from being just text generators into decision-m agents. They can analyze context, plan
[03:48:45] what needs to be done, and then use the appropriate functions to get accurate results all autonomously. In this video,
[03:48:51] we'll explore how to define and register tools in Langen using the tool decorator which turns ordinary Python function
[03:48:58] into callable AI tools. And by the end of this video, you'll understand how this small but powerful addition lays
[03:49:04] the foundation for building LLM power agents that can think, reason, and act, not just chat. So,
[03:49:12] let's go to the implementation.
[03:49:18] So again I'm going to create a new folder folder 14 and then name it tools.
[03:49:26] Inside this I'm going to create a new file tools.py.
[03:49:33] So here uh first of all from line core I'm going to import
[03:49:39] tools.
[03:49:44] So let's create a function here. So our function will simply be a diff multiply function
[03:49:50] uh def multiply a comma b
[03:49:57] and then uh this will result into a integer or I'll also define it in
[03:50:04] this way. So a is supposed to be an integer. B is also supposed to be an integer. And then the product or
[03:50:11] whatever this does uh should return an integer here. So a into b also make sure
[03:50:17] to include the dock string that actually defines what this function does because
[03:50:23] that is important here. So let me write multiply to numbers
[03:50:29] and then I incorporate this using the add tool decorator. Okay. Now this is
[03:50:36] not just a normal function but it is a langen tool function here. So now what I'm
[03:50:42] going to do is I'm going to invoke this multiply function.
[03:50:48] multiply dot invoke and then simply pass a = 3
[03:50:57] and then b = 5. So now
[03:51:03] uh also this should be passed within a within a curly brackets. So let me
[03:51:10] provide it in that way. And then what I can do is I can print
[03:51:15] out the result. Now you can see that this particular tool will be invoked or
[03:51:22] or the tool will invoke the following uh values and then return our
[03:51:27] multiplication result here. So let me run this and it's pretty easy. 5 into 4 is equal
[03:51:34] to 20. So this is how you do it. You can also see uh the function name here. You
[03:51:44] can also see what the dock string in the function says
[03:51:50] dot do string uh or and then you can also see what arguments
[03:51:57] you need to pass inside this particular tool. So let me run this again and then you can see that the function name is
[03:52:05] multiply. You can also see this dock string uh for this second line here. And then the argument you need to pass is
[03:52:12] uh a val a variable a which needs to be an
[03:52:18] integer and variable b which also needs to be an integer.
[03:52:23] What you can also do is you can also see uh the argument schema in terms of JSON.
[03:52:31] So for that model JSON schema
[03:52:37] run this again and then you can see everything
[03:52:44] that we have talked about here in the form of our JSON schema. So this is just
[03:52:51] a basic introduction of using tool in
[03:52:56] lang. In the further videos, we'll explore how we can integrate this with LLM tool. In the last video, we saw how
[03:53:03] the tool decorator helps turn a simple Python function into a callable AI tool, something our language model can
[03:53:08] recognize and use when reasoning. But what if we want more structure and control? What if we want to clearly
[03:53:15] define the input parameters, their types, and even attach wrist rich uh
[03:53:21] detail for each, making the tool more transparent and reliable for the model to use. That's where the structure tool
[03:53:28] comes in. Structure tool takes the concept of tools a step further. It lets
[03:53:33] it wrap a function with a parentic model that defines exactly what inputs the
[03:53:40] tool expects. That means every parameter has a type, a detail, and validation
[03:53:45] rules, ensuring the LLM knows precisely how to use the tool. Think of it like
[03:53:50] giving the model a manual. Instead of saying, "Here's a function. Good luck figuring it out. You're saying here's a
[03:53:57] tool that multiplies two numbers. It require exactly two inteious A and B and here's what they represent. This
[03:54:04] structured approach becomes increas incredibly useful when you're working
[03:54:09] with multiple tools or building complex complex agent agents. It reduces
[03:54:15] ambiguity and helps the LM call your tools correctly every time. So in this video we'll explore how to define a
[03:54:22] structure tool using structure tool from function. How to specify input schemas
[03:54:27] using pyic and why this approach is considered the more scalable and professional way to build tools for L
[03:54:34] images. So let's go to the implementation. So here I'm going to create a new file
[03:54:41] again and then call it structured tool.py.
[03:54:47] Pi here I will have langen
[03:54:54] from langen dot tools I'm going to import structure tool and then from pyic
[03:55:03] we're going to import base model and a field
[03:55:08] okay so I'll first define my parent class here
[03:55:14] so multiply input This will import base model here and
[03:55:19] then I'll have two parameters a it will be an integer. I can write a field for
[03:55:25] this one and then details of this particular field. So description
[03:55:31] equals to the first number to multiply.
[03:55:38] Uh I'll also say this is a required value. So required equals to true. And
[03:55:45] then similarly I'll have the second number here. Uh let me call this second number to
[03:55:54] multiply and then write B here.
[03:56:00] Now what I can do is I can define the multiply function. Multiply function. A
[03:56:07] will accept an integer. B will also accept an integer. And then we'll also return the integer.
[03:56:14] So I will return A into B. I'll define my multiply tool using the
[03:56:20] structure tool function dot from function.
[03:56:26] Uh and inside this I'm going to write my function name which is multiply
[03:56:32] function. Then I'm going to provide a name to this which is multiply.
[03:56:38] Then I'm going to write a dock string for this. So multiplies
[03:56:44] to numbers and then provide the argument schema and then argument schema is based
[03:56:51] on the pion that we have which is multiply input. Okay. Now after this
[03:56:56] what I can do is I can invoke this structure tool multiply
[03:57:02] tool dot invoke then I can pass in the two values. Let's call a = 5 and b can
[03:57:11] be passed as four and from this I can print out my result
[03:57:19] and then run this again. So in our previous video what we had done is we had uh used the tool
[03:57:29] decorator to define the the tool function but in this case we've defined
[03:57:34] a py class for validation too and then uh we have used structure tool to invoke
[03:57:41] this particular function here which follows the validation rules provided provided in the spiral class. So I hope
[03:57:48] you understood the concept and then the major uh difference between tool and
[03:57:53] structured tool in line chain. So far we've talked about how to create individual tools simple functions that
[03:58:00] can that an AI model can understand, interpret and use. But in real world application we rarely rely on just one
[03:58:07] tool. Think about it an AI assistant might need to add numbers, search the web, analyze text or fetch data from a
[03:58:14] database. Each of these capabilities is a separate tool. And when you have several of them, it makes sense to group
[03:58:21] them inside a toolkit. So a toolkit in langen is simply a collection of related
[03:58:26] tools bundled together, usually around a specific purpose. For example, you might have a Mac toolkit for arithmetic
[03:58:33] operation or database toolkit for interacting with SQL or MongoDB or a web toolkit for scraping and browsing data.
[03:58:40] Toolkits make your code modular and organized. Instead of handling each tool individually, you can register, manage
[03:58:48] or pass around entire group of tools as a single unit, especially when integrating them with agents. It's like
[03:58:54] giving your AI or toolbox instead of just one range. Now it can pick the right tool for the right job. In this
[03:59:01] video, we'll explore how to define a simple toolkit in Langchen, how to group multiple tools inside it, and how this
[03:59:08] concept becomes the foundation for building intelligent multiskilled AI agents that can reason and act
[03:59:14] automatically or dynamically. So, let's move to the code.
[03:59:19] So, I'm going to create a new file here called toolkit.py.
[03:59:28] And inside this I'm again going to import tools langen_core
[03:59:35] dot tools import tool and then in a decorator what I'm
[03:59:42] going to do is I'm going to create two tools here. So one will be for addition
[03:59:48] uh which will contain two parameter and integer b will also be an integer
[03:59:54] and then it will also return an integer here. So return a + b. I want to include
[04:00:00] something in the dock string to here and and this will say add two numbers.
[04:00:06] Similarly, I have I'll have another tool uh for multiply and then it will also
[04:00:12] take in two numbers integer B is also integers and then it will reply or
[04:00:20] return an integer itself. I have a dock string says multiply to numbers
[04:00:27] and then it will do return a into B. Okay. So I have multiple tools
[04:00:34] implementation here. So what I'm going to do is I'm going to define a toolkit here called Mac toolkit
[04:00:41] and then uh inside this I'll have a get tools function
[04:00:51] uh which will contain a self keyword and it will return add and then
[04:00:57] multiplication here
[04:01:05] a tool layer. Okay. Then what I'm going to do is I'm going
[04:01:11] to create a toolkit object of this map toolkit class.
[04:01:17] And then inside this I'm going to get my two different uh tools using this
[04:01:24] toolkit. So get tools and then I'm I'm going to print each and
[04:01:29] every tools here. So ptl in tools let's print out the name of the tool. So
[04:01:36] PL dot name and then what we'll do is
[04:01:41] we'll also print out the detail of this tool. So TL
[04:01:48] and then the detail will be TL dot description.
[04:01:54] So if you run this, you'll see that we have bound these two different tools. Okay, something's wrong here. Let me
[04:02:01] check what I've what I've done. Uh
[04:02:09] so I've defined a class.
[04:02:17] I've also defined an object or the matt class. Okay, the object
[04:02:24] definition is wrong here. So, let me run this again. And then using this toolkit class, uh I can get a list of all
[04:02:34] available tools that I have. So, I have two tools here. One is for add and then the other is for multiply. So if I need
[04:02:41] to add one more tool here, let's add one more tool uh called sub which will take in a and b
[04:02:51] both are inteious for now. It will also return an integer and I'll include a
[04:02:58] dock string subtract two numbers and inside this I'll return a minus b
[04:03:05] and then I'll also include this tool here for sub. Then again the toolkit wraps all of my tools and then uh it
[04:03:13] gives me a choice choice choice to choose one among all of
[04:03:19] these tools here. So this is how you implement toolkit. So I'm ending this video right now. I hope you understood
[04:03:25] the concept. We'll further work on tools and then integrate it with LLMs in the upcoming video. Up until now we've seen
[04:03:32] how tools can be created and grouped into toolkits. But what if we want our AI model to actively use those tools
[04:03:39] during conversation? That's where tool binding comes in. So toolbinding is the process of connecting your AI model
[04:03:45] directly with one or more tools. So the model can recognize when a tool should be used and even suggest calling it with
[04:03:52] the right parameters. Essentially the model becomes aware of the tools at
[04:04:03] essentially the model becomes aware of the tools it has. So this is a critical step towards agentic because now the AI
[04:04:10] isn't just generating text it can reason about actions and leverage external functionality to get things done. So for
[04:04:17] example, instead of manually calculating a sum or quering a database, the AI can now identify the need, prepare the
[04:04:24] inputs and suggest invoking the relevant tool. It's important to note that in
[04:04:29] this setup, the model itself doesn't execute the tool. It only proposes a tool call complete with arguments. The
[04:04:36] actual extension is handled programmatically giving uh developers control over safety, validation, and
[04:04:42] workflow. So in this video we'll focus on binding a tool to our LRM seeing how
[04:04:48] the model suggest tool usage and then executing the tools action based on the model suggestion. This is going to be a
[04:04:54] crucial step toward creating intelligent agent that can work or that can do task
[04:05:01] beyond just generating text. So let's go for the implementation.
[04:05:10] So here I'm going to create a new file called a toolbinding
[04:05:15] py. Let's import tool. So from langen core
[04:05:23] lang core dot tools import tool. Okay. What else do I need? Uh we'll import our
[04:05:30] model. So from langen Google geni import uh chat Google generative AI. And then
[04:05:38] I'm going to load my credential from env import
[04:05:43] load env. So let me first load my credential. Let me then load my model.
[04:05:50] My model name is uh Gemini
[04:05:56] 2.5/model. Let me create a tool here. So, so my
[04:06:03] first tool here is add
[04:06:08] which will take in two numbers a and b both will be integers and it will also return an integer value. I'll add a dock
[04:06:16] to this. So add two numbers
[04:06:22] and then uh I will return a + b.
[04:06:30] Okay. So this is done. Uh the tool import I think is wrong. So it should be small do tool. Uh
[04:06:37] what else? Okay, we just one tool. We'll try to bind this with lm. Okay, so llm
[04:06:42] with tools equals to uh model dot
[04:06:49] bind tools. And then I'm going to bind my uh tool into this. Uh so
[04:06:58] we can see uh invoking a query with this particular uh LLM which contains tools
[04:07:06] here. So let's invoke this uh and then uh we'll say can you
[04:07:12] add three with 14 and then basically see the
[04:07:20] result here or uh what I can do is we'll first print out the result and then
[04:07:25] we'll see it accordingly. So if we print out the result then the
[04:07:31] model will not itself start doing a computation but it will look at this particular query and then uh and then
[04:07:39] the dock string and then based on the similar doc string with the query it will understand that now this tool needs
[04:07:45] to be called. Okay let's run this and then you'll get a clear picture of what I'm trying to say here. So if we run
[04:07:51] this
[04:07:59] so as you can see uh the content is actually not an exact answer but it is a
[04:08:07] tool call here or a function call. So it says function call and then and then the function name here is add. The arguments
[04:08:14] are a and b and then and then three and 14 are passed to a and b here. And then
[04:08:21] uh what the model is suggesting is it is suggesting me to call this particular
[04:08:27] function or call this tool to perform the task that is mentioned in the query
[04:08:32] instead of the model doing the computition itself here. So this is how we bind the tools and then the model
[04:08:40] like I said earlier the model does not itself call the tool but it's
[04:08:45] but it actually provides you the suggestion uh suggestion to the user
[04:08:50] that this particular tool uh can be called to perform this particular task that is asked to do here. Up until the
[04:08:58] previous video, we explored tool binding where the AI could suggest using a tool, but it didn't actually execute it.
[04:09:05] That's a big step towards agentic behavior, but it still required a human or programmatic step to run the tool. In
[04:09:11] this video, we're taking it one step further. We're going to enforce the model to actually perform the tool
[04:09:17] operation as part of the workflow. This means the AI isn't just suggesting. It can now actively interact with tools,
[04:09:23] gather results and continue reasoning based on those results. The process
[04:09:28] works like a conversation flow. First, the user provides a query. The model aware of the tools,
[04:09:35] chooses which tool to call and then prepares the arguments. Next, we execute
[04:09:41] the suggested tool call programmatically. And finally, the model
[04:09:46] gets the tools output and integrates it into its response, allowing it to continue the dialogue seamlessly with
[04:09:53] real results. This approach is critical for building truly intelligent agents, one that just doesn't answer questions,
[04:10:00] but can perform task or compute results or fetch data dynamically. So, in this video, our focus will be on implementing
[04:10:06] this inforce tool execution workflow. The model identifies the tool, we run it, and then the AI incorporates the
[04:10:12] output back into the conversation, making the interaction fully functional and autonomous. So let's dive into the
[04:10:18] code.
[04:10:23] So here I'm going to create a new file again call it final tool
[04:10:31] calling.py. Let me import few libraries here. So
[04:10:37] from langen core dot tools I'll import
[04:10:42] tool then from langen core dot messages I'll
[04:10:49] import human message then I'll import my model
[04:10:57] and after that I will uh import
[04:11:04] okay Now I'll import the credential. Then I'll define my model
[04:11:19] chat Google generative AI. So the model name is Gemini 2.5
[04:11:26] flash. And then I'm going to create a tool for basically addition like we did in the
[04:11:32] earlier video. So def add a is going to be an integer. B is also going to be an
[04:11:37] integer and I will return the result as integer. I'll add a dock string add two
[04:11:46] numbers and then based on this I'm going to return a + b.
[04:11:53] Now I'll bind this tool. So, llm with tools equals to llm llm
[04:12:02] dot bind tools and I'm going to find my add
[04:12:09] function tool here. My user query is going to be in u uh add
[04:12:19] 10 and 9. Okay. And then I'm going to convert this
[04:12:26] into human message. So my query is a human message.
[04:12:31] So we'll append this in our message list our query now.
[04:12:39] And then we'll uh print our result. basically call
[04:12:46] uh call the LLM using our
[04:12:51] messages and then print out the result. We'll see the result what happens here.
[04:12:57] Okay. Okay. So if you run this,
[04:13:05] you'll see that it does suggest a uh function call here uh for add to
[04:13:12] basically solve my query. So what I'm going to do here is
[04:13:19] uh I'm going to append this to my my messages. So messages dot append it will
[04:13:29] go inside here also I want to print something here. So let me print out result dot tool calls. I think we have
[04:13:36] tool calls here. Yeah we have tool calls here. So tool call and then we'll invoke
[04:13:42] our first tool call and see what it gives me. So let me run this again.
[04:13:55] So if we print tool call then we can see that this particular tool is reference
[04:14:02] or the add tool is reference. My values are correctly gone to a and bp.
[04:14:08] So now what I'm going to do is I'm again going to remove this and then I'm going to invoke the tool add dot invoke
[04:14:17] result dot tool call zero.
[04:14:23] Then after this I'm going to append append the tool result also inside my
[04:14:30] messages and then the final result will be the llm call again dot invoke and
[04:14:37] then I'll print my final result here. So print final result.
[04:14:47] So let me clear this and then run this again and see what happens.
[04:14:57] Okay, as you can see the sum of 10 and 9 is 19. The content is
[04:15:04] generated. If you only want to see the content then you can see that the content is shown.
[04:15:17] Okay. So what we did here is we did in fact invoke the invoke the tool but we
[04:15:24] did it by ourselves. So basically we took the model suggestion of invoking the add tool. We invoke the add tool and
[04:15:32] then basically uh regenerated our response based on our query year. Uh
[04:15:39] that's not actually uh what we want but I actually wanted to show you how to call the tool here. In the upcoming
[04:15:46] video we will fix this and then we will let the model uh suggest the tool as
[04:15:51] well as call the tool automatically here. So I think I uh you understood the concept. In our previous videos we
[04:15:58] explored how an LLM can suggest tools and how we can execute those tools manually. But what if we want the model
[04:16:05] to actually carry out a multi-step workflow, handle immediate results and inject critical information into
[04:16:11] subsequent tool calls automatically. And that's where the concept of injected tools comes in and it's what we're
[04:16:17] exploring in this video. So imagine the LLM as a helpful assistant with access
[04:16:23] to a set of tools like a calculator or a currency converter. When you ask it a question, it might realize that to
[04:16:30] answer fully, it needs to use more than one tool in a sequence. For example, if you ask like what is the conversion
[04:16:36] factor between USD and NPR and based on that can you convert
[04:16:41] 10 USD to NPR? The assistant first needs to fetch the conversion rate and then apply that rate to the value you want to
[04:16:48] convert. The magic happens through injected tools arguments. Uh this tool
[04:16:55] allows the LLM to recognize that some tools require additional input like the conversion rate for this case which
[04:17:02] might not be explicitly provided by the user but is generated by a previous tool call. Essentially it allows the LLM to
[04:17:09] remember and pass context between tool executions automatically. And here's how
[04:17:14] the workflow unfolds. First of all we start with the LM call. So we send the users query to the LLM bound with tools.
[04:17:21] The LLM analyzes the query and chooses which tools need to be executed. For
[04:17:27] instance, it first calls the get conversion factor tool and then plans to call the convert tool. The second is the
[04:17:34] multi-turn tool edition loop. Here the code then executes each suggested tool.
[04:17:40] When the LM calls convert, it might not know the exact conversion rate yet. This
[04:17:46] is where the injected argument comes into play. uh the program automatically injects the
[04:17:51] conversion rate obtained from the previous tool call ensuring the tool has all the information it needs and the
[04:17:57] final is the feedback or feeding the results back to the LLM. So after each
[04:18:02] tool execution the result is packaged into a tool message and then sent back to the LLM. The LLM then uses these
[04:18:10] output to uh generate the upcoming step or finalize its response. The loop
[04:18:19] continues until LLM produces a final human readable answer in the content field. So this approach is powerful
[04:18:25] because it transforms the LLM from a simple chat model into a true agentic system. It can now plan, execute and
[04:18:32] chain multiple operations together while intelligently handling uh dependencies between them. So in this video we will
[04:18:38] walk through a live implementation of this workflow. We'll see the
[04:18:45] LLM first retrieve the conversion rate and then use it to convert a currency amount and finally produce a complete
[04:18:53] readable answer for the user all automatically. So this pattern is not only limited to uh
[04:19:00] currency conversion. It's a blueprint for like building multi-step AI agent that can handle complex task with
[04:19:07] multiple tools and dependencies. Now let's move on to the implementation.
[04:19:17] So I'm going to create a new file here call it test
[04:19:24] multiple uh tool call.py.
[04:19:33] Okay. So inside this first of all the imports
[04:19:46] first import tool. After that import
[04:19:52] tool message as well as message.
[04:20:03] then load the model
[04:20:12] then env here from inside lang chain to lang
[04:20:19] chain code dot tools I'm also going to in add injected tool ar
[04:20:27] and then uh last one from typing I'm going to import annotated
[04:20:33] Okay. So, first of all, load the credentials. Then I'm going to define a tool here.
[04:20:40] So, this tool will be get conversion rate.
[04:20:46] It will contain two strings. One is base currency.
[04:20:51] It is it will be a string and then the other is target currency.
[04:20:57] It also be a string. and then it will basically return our float value.
[04:21:03] So write something in the dock string. Return the realtime currency
[04:21:11] conversion uh conversion rate from base currency to
[04:21:18] target to target currency
[04:21:25] using some API. And here I'm actually not going to call the API but I'm directly going to use
[04:21:32] this uh static conversion rate. Let's say 140.3.
[04:21:39] Okay. And then I'm going to return this conversion rate from this tool.
[04:21:44] Next I'm going to create another another tool here uh that is going to be named as convert.
[04:21:53] Here I will have a base currency value
[04:21:58] which will be in float and then I'll have a conversion rate
[04:22:05] conversion rate uh which will also be in float but it will be annotated and then
[04:22:15] float and injected tool arc. Basically first of all uh the value will come from
[04:22:22] the other tool to this particular tool here and then it'll also return a float value.
[04:22:31] Write a dock string. Converts a base currency
[04:22:38] value into target currency value
[04:22:44] using
[04:22:49] a conversion rate. Okay. So
[04:22:56] this is going to return base currency value into
[04:23:01] conversion rate. Now we will set up LLM. Uh so lm equal to chat Google generative
[04:23:08] AI. The model is going to be Gemini
[04:23:13] 2.5/model. Then what I'm going to do is I'm going
[04:23:19] to bind these tools. So lm with
[04:23:24] with tools equals to lm dot
[04:23:31] by tools. The first tool is get conversion rate and then this and then the second tool
[04:23:36] is convert. Now I'm going to pass in my query. So my query is going to be what
[04:23:43] is the conversion factor between
[04:23:48] USD and NPR and based on it
[04:23:54] then you convert 10 USD to NPR.
[04:24:03] Okay. So this is going to be my query here.
[04:24:08] Now what I'm going to do is I'm going to include this query as a human message.
[04:24:14] Human message with the content is going to be query
[04:24:19] and I'm going to wrap up wrap this up in a list itself.
[04:24:25] Okay. So let me invoke the tool and then get the AI message from this. So, llm
[04:24:32] with tools dot invoke uh messages and then whatever I get I'm
[04:24:38] going to append it in the messages itself. So, append AI message.
[04:24:47] Okay. Now in step two we go for multi-turn toolation loop. So while uh
[04:24:56] AI AI message dot content
[04:25:02] equals to an empty string and AI dot AI
[04:25:08] message uh dot tool calls
[04:25:14] tool calls. So if something is present in the tool calls what I'm going to do is
[04:25:22] I am going to create a list of tool message tool messages
[04:25:29] and then based on this I'm going to execute all suggested tool. So for tool
[04:25:35] call in AI dot sorry AI message dot tool
[04:25:42] calls uh I'll first call in the tool name which
[04:25:47] is going to come from tool call name.
[04:25:54] I will also try to get its ID. So tool id equals to tool call
[04:26:02] id and then I'll get the arguments. So tool arcs equals to tool call
[04:26:10] uh tool call and then arx here. Okay. So
[04:26:16] let me print every tool that I've got. So executing
[04:26:25] executing tool and
[04:26:31] here will be my tool name tool name with arcs
[04:26:38] and then here will be my tool ars. So this will be made as an f string here.
[04:26:46] Okay, this will be or I can just do this and then this is
[04:26:52] going to be my string. So now if the tool name is equals to
[04:27:02] get conversion rate
[04:27:08] then I'll invoke that function here dot invoke.
[04:27:15] I invoke that tool and then I'll get the tool output
[04:27:21] in the form of a string and then that will be my conversion
[04:27:27] rate.
[04:27:32] So else uh if the tool name is equals to convert.
[04:27:42] So if the tool name is directly convert then what we need to do is we need to uh
[04:27:49] first inject the conversion rate from the previous result step. So if uh
[04:27:56] conversion rate not in tool arcs
[04:28:04] ensure the conversion rate is actually uh defined from the get conversion factor
[04:28:10] runs. So if uh
[04:28:16] conversion rate not in locals
[04:28:24] or our conversion rate is none.
[04:28:31] Then what we need to do is we need to raise a value error uh saying that
[04:28:39] conversion rate is required but it is not found.
[04:28:48] else what we need to do we need to get the conversion rate from tool arcs
[04:28:55] conversion rate equal to conversion rate
[04:29:05] and then based on this I need to execute my tools so converted value equals to
[04:29:11] convert dot invoke to ars
[04:29:17] and then tool output can be converted to a string value using
[04:29:26] str function. Okay, if none of this is possible,
[04:29:32] we'll do tool output equals to unknown
[04:29:41] unknown tool and then we will say tool name
[04:29:48] Okay, most of our work is done. Uh what we need to do is now we need to since
[04:29:54] every condition returns a tool message to us. So what we're going to do is
[04:29:59] we're going to create a tool message. It will be wrapped inside this tool message
[04:30:05] here. The content is going to be tool output
[04:30:11] tool output. And then the tool called id
[04:30:18] ID will come from the tool ID and I'm going to append uh tool messages
[04:30:26] dot append tool message
[04:30:32] tool msg and then and then now
[04:30:39] I'm going to append everything to our to our messages here. So still I means I I
[04:30:46] am supposed to be inside this loop here. Uh okay. I think I think I did some
[04:30:52] mistake here. So this has to go inside the Y loop
[04:31:02] inside uh inside the Y loop. And then now what I'm going to do is I'm going to
[04:31:08] messages extend.append happened you can use anything tool messages
[04:31:16] then I'll again prompt for AI message and then invoke the model
[04:31:22] with with the updated messages and then I'm again going to
[04:31:28] append it in my AI message okay so finally
[04:31:38] we have we already should have obtained our result inside this particular part
[04:31:43] here. So what I'm going to do is I'm going to take out the last result. So messages minus one
[04:31:53] minus one and then I'm going to print the final result dot content. So let's
[04:31:59] see my query is what is the conversion factor between USD and NPR and based on it can you convert it from 10 USD to
[04:32:05] NPR. So if you run this,
[04:32:11] let's see what happens. So the model is called
[04:32:17] the model is initialized.
[04:32:23] Uh so okay uh the conversion rate between USD and NPR is 140.3. This is
[04:32:31] done using this get conversion rate tool. And then from that after that we
[04:32:36] call this convert tool uh with the base currency value of 10 and then the exchange rate or conversion rate of uh
[04:32:44] 140.3 and then 10 USD is equivalent to 1403 NPR. So this is the result. So here
[04:32:51] we've not only uh integrated multiple LLMs into it into sorry multiple tools
[04:32:59] into an LLM but we've also uh use this concept of injected tool where the
[04:33:06] result of the previous tool will be the input for the upcoming tool. So I hope you understand
[04:33:12] the concept of injected tool uh and then you had fun implementing this particular
[04:33:18] code here. If you have any problem do comment down below and I'll try to help you out. And in the
[04:33:27] next few videos we're going to see AI agents demo using Langen. So we are
[04:33:33] almost at the end of this langen series